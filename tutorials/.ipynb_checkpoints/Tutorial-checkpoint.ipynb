{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c1f816dc",
   "metadata": {},
   "source": [
    "import timm\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import ipdb\n",
    "import timm\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch_pruning as tp\n",
    "import torchvision.models as models\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# check device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class config:\n",
    "    lr = 1e-4\n",
    "    n_classes = 10\n",
    "    epochs = 2\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02b3321b",
   "metadata": {},
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10(root='data/', download=True, transform=transform_train)\n",
    "valid_dataset = CIFAR10(root='data/',  download=True,train=False, transform=transform_test)\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1cc3e517",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def training(model, train_loader, valid_loader):\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    model.train()\n",
    "    validation_acc = 0\n",
    "    ########### Train  ###############\n",
    "    for ep in range(config.epochs):\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            optim.zero_grad()\n",
    "            image, label = data\n",
    "            image, label = image.to(config.device), label.to(config.device)\n",
    "            out = model(image)\n",
    "            loss = criterion(out, label)\n",
    "            acc = torch.argmax(out, 1) - label\n",
    "            running_acc+= len(acc[acc==0])\n",
    "            running_loss+= loss.item() * label.size(0)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        epoch_loss = running_loss/ len(train_loader.dataset)\n",
    "        train_epoch_acc = running_acc/ len(train_loader.dataset)\n",
    "        \n",
    "        \n",
    "        ########### Validate #############\n",
    "\n",
    "\n",
    "        print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {ep+1}\\t'\n",
    "                  f'Train loss: {epoch_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_epoch_acc:.2f}\\t')\n",
    "        val_acc = validate(model, valid_loader)\n",
    "        if(validation_acc< val_acc):\n",
    "            validation_acc = val_acc\n",
    "            torch.save(model.state_dict(), './weights/dl-x25'+str(validation_acc)+'.pth')\n",
    "            \n",
    "    print(\"Final Validation Accuracy:\", validation_acc, \"\\n \\n\")\n",
    "    return model\n",
    "\n",
    "def validate(model, valid_loader):\n",
    "    val_running_acc = 0\n",
    "    model.eval()\n",
    "    for batch_idx, data in enumerate(valid_loader):\n",
    "\n",
    "        image, label = data\n",
    "        image, label = image.to(config.device), label.to(config.device)\n",
    "        out = model(image)\n",
    "        acc = torch.argmax(out, 1) - label\n",
    "        val_running_acc+= len(acc[acc==0])\n",
    "    val_epoch_acc = val_running_acc/ len(valid_loader.dataset)\n",
    "\n",
    "\n",
    "    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "              f'Valid accuracy: {100 * val_epoch_acc:.2f}')\n",
    "    return val_epoch_acc\n",
    "        \n",
    "            \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a156ef01",
   "metadata": {},
   "source": [
    "number_conv_layers = 0\n",
    "\n",
    "\n",
    "def universal_layer_identifier(identification, module, module_name):\n",
    "    if(identification == None):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            return True\n",
    "    else:\n",
    "        if (identification in module_name):\n",
    "            return True\n",
    "def universal_get_layer_id_pruning(model, pruning_type, pruning_percent, identification=None): #identification = none\n",
    "    if(pruning_type == 'L1'):\n",
    "        strategy  = tp.strategy.L1Strategy()\n",
    "    elif(pruning_type == 'L2'):\n",
    "        strategy  = tp.strategy.L2Strategy()\n",
    "    channels_pruned = []\n",
    "    \n",
    "    def find_instance(obj):\n",
    "        if isinstance(obj, nn.Conv2d):\n",
    "            pruning_idx = strategy(obj.weight, amount = pruning_percent)\n",
    "            channels_pruned.append(pruning_idx)\n",
    "            return None\n",
    "        elif isinstance(obj, list):\n",
    "            for internal_obj in obj:\n",
    "                find_instance(internal_obj)\n",
    "        elif (hasattr(obj, '__class__')):\n",
    "            for internal_obj in obj.children():\n",
    "                find_instance(internal_obj)\n",
    "        elif isinstance(obj, OrderedDict):\n",
    "            for key, value in obj.items():\n",
    "                find_instance(value)\n",
    "\n",
    "    find_instance(model)\n",
    "\n",
    "    channels_pruned = np.asarray(channels_pruned, dtype=object)\n",
    "    return channels_pruned\n",
    "\n",
    "\n",
    "\n",
    "def universal_filter_pruning(model, input_shape, channels_pruned, identification=None):\n",
    "    DG = tp.DependencyGraph()\n",
    "    DG.build_dependency(model, example_inputs= torch.randn(input_shape).to(config.device))\n",
    "\n",
    "    layer_id = 0\n",
    "\n",
    "    def find_instance(obj):\n",
    "        if isinstance(obj, nn.Conv2d):\n",
    "            global number_conv_layers\n",
    "            number_conv_layers+=obj.out_channels\n",
    "            pruning_plan = DG.get_pruning_plan(obj, tp.prune_conv_out_channel, idxs=channels_pruned[layer_id])\n",
    "            pruning_plan.exec()\n",
    "            return None\n",
    "        elif isinstance(obj, list):\n",
    "            for internal_obj in obj:\n",
    "                find_instance(internal_obj)\n",
    "        elif (hasattr(obj, '__class__')):\n",
    "            for internal_obj in obj.children():\n",
    "                find_instance(internal_obj)\n",
    "        elif isinstance(obj, OrderedDict):\n",
    "            for key, value in obj.items():\n",
    "                find_instance(value)\n",
    "\n",
    "    find_instance(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c65198af",
   "metadata": {},
   "source": [
    "from thop import profile, clever_format\n",
    "\n",
    "\n",
    "def pruner(model,experiment_name, config, input_dims, pruning_stratergy, pruning_percent,  train_loader, valid_loader):\n",
    "    original_model = model\n",
    "    input = torch.randn((config.batch_size, )+ input_dims).to(config.device)\n",
    "    \n",
    "    macs_original, params_original = profile(original_model, inputs=(input, ))\n",
    "#     macs_original, params_original = clever_format([macs_original, params_original], \"%.3f\")\n",
    "    \n",
    "    torch.save(original_model.state_dict(), './weights/'+experiment_name+'.pth')\n",
    "    print(\"\\n \\n Original Validation Accuracy: \\n \\n\")\n",
    "    validate(model, valid_loader)\n",
    "    \n",
    "    channels_pruned = universal_get_layer_id_pruning(model, pruning_stratergy, pruning_percent)\n",
    "    print(\"\\n \\n ################################# Post Purning ################################# \\n \\n \")\n",
    "    print(\"Original Conv Layers in the Model:\", number_conv_layers ,\"\\n Number of Layers Selected:\", len(channels_pruned), \"\\n Number of Filters Pruned:\",sum([len(x) for x in channels_pruned]))\n",
    "    pruned_model = universal_filter_pruning(model, (config.batch_size,)+input_dims, channels_pruned).to(config.device)\n",
    "    pruned_model = training(pruned_model, train_loader, valid_loader)\n",
    "    torch.save(pruned_model, './weights/'+experiment_name+'_pruned_model.pth')\n",
    "    torch.save(pruned_model.state_dict(), './weights/'+experiment_name+'_pruned.pth')\n",
    "    print(\"\\n \\n Pruned Validation Accuracy: \\n \\n\")\n",
    "    validate(pruned_model, valid_loader)\n",
    "    \n",
    "    print(\"\\n \\n ################################# MAC's and Parameters Comparison #################################\")\n",
    "\n",
    "    \n",
    "    macs_pruned, params_pruned = profile(pruned_model, inputs=(input, ))\n",
    "\n",
    "    print(\"\\n \\n Original Model MAC's and Params:\",macs_original, params_original)\n",
    "    print(\"Pruned Model MAC's and Params:\",macs_pruned, params_pruned )\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "99030693",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "resnet = timm.create_model('resnet18', num_classes=10).to(\"cuda\")\n",
    "  # Pretrained Model\n",
    "resnet.load_state_dict(torch.load('/home/beast/Downloads/resnet18.pth'))\n",
    "\n",
    "\n",
    "pruner(resnet,\"resnet18\", config,(3,32,32), \"L2\", 0.06,  train_loader, valid_loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4c00d8",
   "metadata": {},
   "source": [
    "                              ######### Original Model  #########\n",
    "# original_model = timm.create_model('resnet18', num_classes=10).to(\"cuda\")\n",
    "# original_model.load_state_dict(torch.load('./weights/resnet18.pth'), strict=False)\n",
    "# validate(original_model, valid_loader)\n",
    "# summary(original_model,(3,32,32))\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00386346",
   "metadata": {},
   "source": [
    "                                ######### Pruned Model  #########\n",
    "# pruned_model = torch.load(\"./weights/resnet18_pruned_model.pth\")\n",
    "# pruned_model.load_state_dict(torch.load('./weights/resnet18_pruned.pth'), strict=False)\n",
    "# validate(pruned_model, valid_loader)\n",
    "# summary(pruned_model,(3,32,32))\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e0f018d9",
   "metadata": {},
   "source": [
    "pruned_model = torch.load(\"./weights/resnet18_pruned_model.pth\").to(\"cuda\")\n",
    "pruned_model.eval()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8846daa0",
   "metadata": {},
   "source": [
    "                              ######## Pick the layers to fuse ########\n",
    "layer_list = []\n",
    "layer_fuse_list= []\n",
    "flag =-1\n",
    "\n",
    "for name, layer in pruned_model.named_modules():\n",
    "    \n",
    "    if(isinstance(layer, nn.ReLU)):\n",
    "            \n",
    "        layer_fuse_list.append(name)\n",
    "        layer_list.append(layer_fuse_list)\n",
    "        layer_fuse_list = []\n",
    "\n",
    "    if ((len(layer_fuse_list)<2) and (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d))):\n",
    "        \n",
    "        layer_fuse_list.append(name)\n",
    "        \n",
    "        \n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "pruned_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "pruned_model_prepared = torch.quantization.prepare_qat(pruned_model.train())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1248581f",
   "metadata": {},
   "source": [
    "pruned_model_prepared = training(pruned_model_prepared, train_loader, valid_loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef65df4b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "pruned_model_prepared = pruned_model_prepared.to(\"cpu\")\n",
    "pruned_model_prepared.eval()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "423d90f3",
   "metadata": {},
   "source": [
    "pruned_model_int8 = torch.quantization.convert(pruned_model_prepared)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "13cf6d7f",
   "metadata": {},
   "source": [
    "torch.save(pruned_model_int8.state_dict(), './weights/pruned_model_int8-weights.pth')\n",
    "torch.save(pruned_model_int8, './weights/pruned_model_int8.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "813ee92a",
   "metadata": {},
   "source": [
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)\n",
    "        "
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
