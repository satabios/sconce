{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47eefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb0e913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobilenetv2_035',\n",
       " 'mobilenetv2_050',\n",
       " 'mobilenetv2_075',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_050',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*mobilenet*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c691b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('mobilenetv3_small_100', num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusing_layers = [\n",
    "            'Conv2d',\n",
    "            'BatchNorm2d',\n",
    "            'ReLU',\n",
    "            'Linear',\n",
    "            'BatchNorm1d',\n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "def detect_sequences(lst):\n",
    "\n",
    "            i = 0\n",
    "            while i < len(lst):\n",
    "               \n",
    "                if i + 2 < len(lst) and [l for l in lst[i : i + 3]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    test_layer = layer_shapes[i : i + 2]\n",
    "                    if(np.all(test_layer == test_layer[0])):\n",
    "                        mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                        )\n",
    "                        i += 3\n",
    "                \n",
    "                elif i + 1 < len(lst) and [l for l in lst[i : i + 2]] == [\n",
    "                    fusing_layers[0],\n",
    "                    fusing_layers[1],\n",
    "                ]:\n",
    "                    test_layer = layer_shapes[i : i + 2]\n",
    "                    if(np.all(test_layer == test_layer[0])):\n",
    "                        \n",
    "                        mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                            np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                        )\n",
    "                        i += 2\n",
    "                # if i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[0], fusing_layers[2]]:\n",
    "                #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "                #     i += 2\n",
    "                # elif i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[1], fusing_layers[2]]:\n",
    "                #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "                #     i += 2\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i : i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[2],\n",
    "                ]:\n",
    "                    mapped_layers['Linear_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                elif i + 1 < len(lst) and [l for l in lst[i : i + 2]] == [\n",
    "                    fusing_layers[3],\n",
    "                    fusing_layers[4],\n",
    "                ]:\n",
    "                    mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "def get_all_layers(model, parent_name=''):\n",
    "    layers = []\n",
    "    for name, module in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        layers.append((reformat_layer_name(full_name), module))\n",
    "        if isinstance(module, nn.Module):\n",
    "            layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def reformat_layer_name(str_data):\n",
    "    try:\n",
    "    \n",
    "        split_data = str_data.split('.')\n",
    "        for ind in range(len(split_data)):\n",
    "            data = split_data[ind]\n",
    "            if(data.isdigit()):\n",
    "                split_data[ind] = \"[\"+data+\"]\"\n",
    "        final_string = '.'.join(split_data)\n",
    "\n",
    "        iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "        indices = [m.start(0)+1 for m in iters_a]\n",
    "        iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "        indices.extend([m.start(0) for m in iters])\n",
    "\n",
    "        final_string = list(final_string)\n",
    "        for ind in indices:\n",
    "            final_string.pop(ind)\n",
    "        str_data = ''.join(final_string)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return str_data\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "    \n",
    "\n",
    "    def register_hook(module, layer_name, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx  # Add this line to access the outer module_idx variable\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "\n",
    "            m_key = reformat_layer_name(all_layers[module_idx][0])\n",
    "          \n",
    "                \n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook for each layer\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, layer_name, module_idx)\n",
    "\n",
    "    # make a forward pass\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66bd35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(self.model)\n",
    "all_layers = get_all_layers(model)\n",
    "model_summary = summary_string_fixed(model, (3, 64, 64))#, device=\"cuda\")\n",
    "\n",
    "name_type_shape = []\n",
    "for key in model_summary.keys():\n",
    "    data = model_summary[key]\n",
    "    if (\"weight_shape\" in data.keys()):\n",
    "        name_type_shape.append([key, data['type'], data['weight_shape'][0]])      \n",
    "name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "name_list = name_type_shape[:,0]\n",
    "layer_types = name_type_shape[:,1]\n",
    "layer_types[layer_types=='BatchNormAct2d'] = 'BatchNorm2d'\n",
    "layer_shapes = name_type_shape[:,2]\n",
    "mapped_layers = {'model_layer':[],'Conv2d_BatchNorm2d_ReLU':[],'Conv2d_BatchNorm2d':[],'Linear_ReLU':[],'Linear_BatchNorm1d':[]}\n",
    "        \n",
    "detect_sequences(layer_types)\n",
    "\n",
    "for keys,value in mapped_layers.items():\n",
    "    mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "    \n",
    "mapped_layers['model_layer'] = name_type_shape[:,:2]\n",
    "# self.mapped_layers = mapped_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b41c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folding\n",
    "#CWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # iterate through conv layers\n",
    "# for i_conv in range(len(all_convs) - 1):\n",
    "#     # each channel sorting index, we need to apply it to:\n",
    "#     # - the output dimension of the previous conv\n",
    "#     # - the previous BN layer\n",
    "#     # - the input dimension of the next conv (we compute importance here)\n",
    "#     prev_conv = getattr(model,all_convs[i_conv])\n",
    "#     prev_bn = getattr(model,all_bns[i_conv])\n",
    "#     next_conv = getattr(model,all_convs[i_conv + 1])\n",
    "#     # note that we always compute the importance according to input channels\n",
    "#     importance = self.get_input_channel_importance(next_conv.weight)\n",
    "#     # sorting from large to small\n",
    "#     sort_idx = torch.argsort(importance, descending=True)\n",
    "\n",
    "#     # apply to previous conv and its following bn\n",
    "#     prev_conv.weight.copy_(\n",
    "#         torch.index_select(prev_conv.weight.detach(), 0, sort_idx)\n",
    "#     )\n",
    "#     for tensor_name in [\"weight\", \"bias\", \"running_mean\", \"running_var\"]:\n",
    "#         tensor_to_apply = getattr(prev_bn, tensor_name)\n",
    "#         tensor_to_apply.copy_(\n",
    "#             torch.index_select(tensor_to_apply.detach(), 0, sort_idx)\n",
    "#         )\n",
    "\n",
    "#     # apply to the next conv input (hint: one line of code)\n",
    "#     ##################### YOUR CODE STARTS HERE #####################\n",
    "#     next_conv.weight.copy_(\n",
    "#         torch.index_select(next_conv.weight.detach(), 1, sort_idx)\n",
    "#     )\n",
    "\n",
    "# return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff82385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
