{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class Config:\n",
    "    BATCH_SIZE = 512\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 1\n",
    "    QAT_EPOCHS = 5\n",
    "    WEIGHT_BITS = 8\n",
    "    ACTIVATION_BITS = 8\n",
    "    PATIENCE = 3\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeSTE(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Straight-Through Estimator (STE) for quantization.\n",
    "    During the forward pass, the input tensor is quantized.\n",
    "    During the backward pass, the gradients are passed through as if the quantization operation was the identity function.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor: torch.Tensor, bit_width: int):\n",
    "        # Quantize the tensor\n",
    "        max_val = tensor.abs().max()\n",
    "        scale = max_val / (float(2 ** (bit_width - 1) - 1)) if max_val != 0 else 1.0\n",
    "        q_tensor = torch.round(tensor / scale).clamp(\n",
    "            min=-(2 ** (bit_width - 1)),\n",
    "            max=(2 ** (bit_width - 1) - 1)\n",
    "        )\n",
    "        # Save scale for backward pass\n",
    "        ctx.save_for_backward(scale)\n",
    "        return q_tensor.to(torch.int8), scale\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_q_tensor, grad_scale):\n",
    "        # During backward pass, pass the gradient through as if the quantization was the identity function\n",
    "        scale, = ctx.saved_tensors\n",
    "        grad_tensor = grad_q_tensor / scale\n",
    "        return grad_tensor, None  # No gradient for bit_width\n",
    "\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, bit_width: int = 8, \n",
    "                 act_bit_width: int = 8, device: str = 'cpu'):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "            \n",
    "        self.bit_width = bit_width\n",
    "        self.act_bit_width = act_bit_width\n",
    "        self.device = device\n",
    "\n",
    "        self.is_conv = isinstance(layer, nn.Conv2d)\n",
    "        if self.is_conv:\n",
    "            self.stride = layer.stride\n",
    "            self.padding = layer.padding\n",
    "\n",
    "        self.weight = nn.Parameter(layer.weight.data.detach().clone()).to(device)\n",
    "        self.bias = nn.Parameter(layer.bias.data.detach().clone()).to(device) if layer.bias is not None else None\n",
    "    \n",
    "        self.register_buffer('weight_scale', torch.tensor(1.0).to(device))\n",
    "        self.register_buffer('act_scale', torch.tensor(1.0).to(device))\n",
    "        self.register_buffer('qweight', None)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Quantize weights using STE\n",
    "        if self.qweight is None or self.training:\n",
    "            self.qweight, self.weight_scale = QuantizeSTE.apply(self.weight, self.bit_width)\n",
    "            self.qweight = self.qweight.to(self.device)\n",
    "        \n",
    "        # Perform the linear or convolutional operation\n",
    "        if self.is_conv:\n",
    "            x = F.conv2d(x, self.qweight.float() * self.weight_scale, self.bias, stride=self.stride, padding=self.padding)\n",
    "        else:\n",
    "            x = F.linear(x, self.qweight.float() * self.weight_scale, self.bias)\n",
    "        \n",
    "        # Quantize activations using STE\n",
    "        if self.act_bit_width is not None:\n",
    "            x, self.act_scale = QuantizeSTE.apply(x, self.act_bit_width)\n",
    "            x = x.float() * self.act_scale\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_conv:\n",
    "            return f\"QuantizedConv2d(in_channels={self.weight.shape[1]}, out_channels={self.weight.shape[0]}, bias={self.bias is not None})\"\n",
    "        else:\n",
    "            return f\"QuantizedLinear(in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "def get_data_loaders():\n",
    "    image_size = 32\n",
    "    transforms = {\n",
    "        \"train\": Compose([\n",
    "            RandomCrop(image_size, padding=4),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "        ]),\n",
    "        \"test\": ToTensor(),\n",
    "    }\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transforms['train'], download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transforms['test'], download=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Model training\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, \n",
    "                criterion: nn.Module, optimizer: optim.Optimizer, qat: bool = False):\n",
    "    epochs = Config.EPOCHS if not qat else Config.QAT_EPOCHS\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model at the end of each epoch\n",
    "        accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace layers with quantized versions\n",
    "def replace_layers(model: nn.Module, device: str):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear,nn.Conv2d)):\n",
    "            quant_layer = QuantizedLinear(module, \n",
    "                                        bit_width=Config.WEIGHT_BITS,\n",
    "                                        act_bit_width=Config.ACTIVATION_BITS,\n",
    "                                        device=device)\n",
    "            setattr(model, name, quant_layer)\n",
    "        else:\n",
    "            replace_layers(module, device)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "class VGG(nn.Module):\n",
    "      ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "    \n",
    "      def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "    \n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "          layers.append((f\"{name}{counts[name]}\", layer))\n",
    "          counts[name] += 1\n",
    "    \n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "          if x != 'M':\n",
    "            # conv-bn-relu\n",
    "            add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "            add(\"bn\", nn.BatchNorm2d(x))\n",
    "            add(\"relu\", nn.ReLU(True))\n",
    "            in_channels = x\n",
    "          else:\n",
    "            # maxpool\n",
    "            add(\"pool\", nn.MaxPool2d(2))\n",
    "    \n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "    \n",
    "      def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "    \n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "    \n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      " Original Model: VGG(\n",
      "  (backbone): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace=True)\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu5): ReLU(inplace=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace=True)\n",
      "    (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu7): ReLU(inplace=True)\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Original Model Accuracy: 92.95%\n"
     ]
    }
   ],
   "source": [
    "#Load Dataloader\n",
    "train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "#Load Pre Trained Model and Weights\n",
    "model_path = 'vgg.cifar.pretrained.pth'\n",
    "model = VGG().cuda()\n",
    "checkpoint = torch.load(model_path, weights_only=True)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "print(f\"\\n Original Model: {model}\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Original Model Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model: VGG(\n",
      "  (backbone): Sequential(\n",
      "    (conv0): QuantizedConv2d(in_channels=3, out_channels=64, bias=False)\n",
      "    (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (conv1): QuantizedConv2d(in_channels=64, out_channels=128, bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): QuantizedConv2d(in_channels=128, out_channels=256, bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (conv3): QuantizedConv2d(in_channels=256, out_channels=256, bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv4): QuantizedConv2d(in_channels=256, out_channels=512, bias=False)\n",
      "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace=True)\n",
      "    (conv5): QuantizedConv2d(in_channels=512, out_channels=512, bias=False)\n",
      "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu5): ReLU(inplace=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv6): QuantizedConv2d(in_channels=512, out_channels=512, bias=False)\n",
      "    (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace=True)\n",
      "    (conv7): QuantizedConv2d(in_channels=512, out_channels=512, bias=False)\n",
      "    (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu7): ReLU(inplace=True)\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): QuantizedLinear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Apply QAT by replacing layers\n",
    "qat_model = copy.deepcopy(model)\n",
    "replace_layers(qat_model, Config.DEVICE)\n",
    "print(f\"Quantized Model: {qat_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:04<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Accuracy: 92.94%\n"
     ]
    }
   ],
   "source": [
    "#Run Quantization Aware Training\n",
    "qat_model.to(Config.DEVICE)  # Move the model to the specified device\n",
    "optimizer = optim.Adam(qat_model.parameters(), lr=Config.LEARNING_RATE)\n",
    "train_model(qat_model, train_loader, test_loader, criterion, optimizer, qat=True)\n",
    "\n",
    "accuracy = evaluate_model(qat_model, test_loader)\n",
    "print(f\"Quantized Model Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLayer: backbone.conv0,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv1,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv2,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv3,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv4,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv5,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv6,  Qweight dtype: torch.int8\n",
      "QLayer: backbone.conv7,  Qweight dtype: torch.int8\n",
      "QLayer: classifier,  Qweight dtype: torch.int8\n"
     ]
    }
   ],
   "source": [
    "#Inspect the weight dtypes of the QAT Model\n",
    "for name, module in qat_model.named_modules():\n",
    "    if isinstance(module, QuantizedLinear):\n",
    "        print(f\"QLayer: {name},  Qweight dtype: {module.qweight.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
