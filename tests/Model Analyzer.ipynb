{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0da01c",
   "metadata": {},
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df72568",
   "metadata": {},
   "source": [
    "\n",
    "def name_fixer(names):\n",
    "    \"\"\"\n",
    "    Fix the names by removing the indices in square brackets.\n",
    "    Args:\n",
    "      names (list): List of names.\n",
    "\n",
    "    Returns:\n",
    "      list: List of fixed names.\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    for string in names:\n",
    "        matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "        pop_list = [m.start(0) for m in matches]\n",
    "        pop_list.sort(reverse=True)\n",
    "        if len(pop_list) > 0:\n",
    "            string = list(string)\n",
    "            for pop_id in pop_list:\n",
    "                string.pop(pop_id)\n",
    "            string = ''.join(string)\n",
    "        return_list.append(string)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "\n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                # conv-bn-relu\n",
    "                add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add(\"bn\", nn.BatchNorm2d(x))\n",
    "                add(\"relu\", nn.ReLU(True))\n",
    "                in_channels = x\n",
    "            else:\n",
    "                # maxpool\n",
    "                add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "\n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "fusing_layers = [\n",
    "    'Conv2d',\n",
    "    'BatchNorm2d',\n",
    "    'ReLU',\n",
    "    'Linear',\n",
    "    'BatchNorm1d',\n",
    "]\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_all_layers(model, parent_name=''):\n",
    "    layers = []\n",
    "    for name, module in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        test_name = \"model.\" + full_name\n",
    "        try:\n",
    "            eval(test_name)\n",
    "            layers.append((full_name, module))\n",
    "        except:\n",
    "            layers.append((reformat_layer_name(full_name), module))\n",
    "        if isinstance(module, nn.Module):\n",
    "            layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def reformat_layer_name(str_data):\n",
    "    try:\n",
    "        split_data = str_data.split('.')\n",
    "        for ind in range(len(split_data)):\n",
    "            data = split_data[ind]\n",
    "            if (data.isdigit()):\n",
    "                split_data[ind] = \"[\" + data + \"]\"\n",
    "        final_string = '.'.join(split_data)\n",
    "\n",
    "        iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "        indices = [m.start(0) + 1 for m in iters_a]\n",
    "        iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "        indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "        final_string = list(final_string)\n",
    "        final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "        str_data = ''.join(final_string)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return str_data\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_input_channel_importance(weight):\n",
    "    importances = []\n",
    "    for i_c in range(weight.shape[1]):\n",
    "        channel_weight = weight.detach()[:, i_c]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "\n",
    "def get_importance(layer, sparsity):\n",
    "    sorted_indices = torch.argsort(get_input_channel_importance(layer.weight), descending=True)\n",
    "    n_keep = int(round(len(sorted_indices) * (1.0 - sparsity)))\n",
    "    indices_to_keep = sorted_indices[:n_keep]\n",
    "    return indices_to_keep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_cwp(model, pruning_ratio_list):\n",
    "    \n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "\n",
    "    def get_layer_name(obj):\n",
    "        if isinstance(obj, list):\n",
    "            layer_list = []\n",
    "            for internal_layer in obj:\n",
    "                layer_list.append(eval(internal_layer.replace('model', 'pruned_model')))\n",
    "            return layer_list\n",
    "        else:\n",
    "            nonlocal pruned_model\n",
    "            return eval(obj.replace('model', 'pruned_model'))\n",
    "\n",
    "    for list_ind in range(len(possible_indices_ranges)):\n",
    "        sparsity = pruning_ratio_list[list_ind]\n",
    "        layer_list = np.asarray(possible_indices_ranges[list_ind])\n",
    "\n",
    "        prev_conv = get_layer_name(layer_list[0, 0])\n",
    "        prev_bn = get_layer_name(layer_list[0, 1])\n",
    "        next_convs = [c for c in get_layer_name(list(layer_list[1:, 0]))]\n",
    "        next_bns = [b for b in get_layer_name(list(layer_list[1:-1, 1]))]  # Avoid last 0\n",
    "\n",
    "        if (len(next_bns) == 0):\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, []])\n",
    "        else:\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, next_bns])\n",
    "\n",
    "        importance_list_indices = get_importance(layer=next_convs[-1], sparsity=sparsity)\n",
    "\n",
    "        def prune_bn(layer, importance_list_indices):\n",
    "\n",
    "            layer.weight.set_(layer.weight.detach()[importance_list_indices])\n",
    "            layer.bias.set_(layer.bias.detach()[importance_list_indices])\n",
    "            layer.running_mean.set_(layer.running_mean.detach()[importance_list_indices])\n",
    "            layer.running_var.set_(layer.running_var.detach()[importance_list_indices])\n",
    "\n",
    "        for prev_layer, next_layers in iter_layers:\n",
    "            if(prev_layer != 0): #No BatchNorm Used:\n",
    "                \n",
    "                if (str(type(prev_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                    prune_bn(prev_layer, importance_list_indices)\n",
    "                else:\n",
    "                    prev_layer.weight.set_(prev_conv.weight.detach()[importance_list_indices, :])\n",
    "                    if prev_layer.bias is not None:\n",
    "                                bias_shape = prev_layer.weight.shape[0]\n",
    "                                prev_layer.bias = nn.Parameter(prev_layer.bias[:bias_shape])\n",
    "                    \n",
    "\n",
    "            if (len(next_layers) != 0):\n",
    "                for next_layer in next_layers:\n",
    "                    if (str(type(next_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                        prune_bn(next_layer, importance_list_indices)\n",
    "                    else:\n",
    "                        if (next_layer.weight.shape[1] == 1):\n",
    "                            \n",
    "\n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[importance_list_indices, :])\n",
    "                            number_of_channels = len(importance_list_indices)\n",
    "                            next_layer.groups = number_of_channels\n",
    "                           \n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "     \n",
    "                              \n",
    "                        else:\n",
    "                            \n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[:, importance_list_indices])\n",
    "#                             next_layer.groups = len(importance_list_indices)\n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "                \n",
    "#                             if(next_layer.bias!=None):\n",
    "#                                 next_layer.bias.set_(next_layer.bias[:next_layer.weight.shape[1]])\n",
    "#                                 print(len(next_layer.bias))\n",
    "\n",
    "    return pruned_model, model\n",
    "\n",
    "\n",
    "def layer_mapping(model):\n",
    "    all_layers = get_all_layers(model)\n",
    "    model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')  # , device=\"cuda\")\n",
    "\n",
    "    name_type_shape = []\n",
    "    for key in model_summary.keys():\n",
    "        data = model_summary[key]\n",
    "        if (\"weight_shape\" in data.keys()):\n",
    "            name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "        #     else:\n",
    "    #         name_type_shape.append([key, data['type'], 0 ])\n",
    "    name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "    name_list = name_type_shape[:, 0]\n",
    "\n",
    "    r_name_list = np.asarray(name_list)\n",
    "    random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "    test_name_list = r_name_list[random_picks]\n",
    "    eval_hit = False\n",
    "    for layer in test_name_list:\n",
    "        try:\n",
    "            eval(layer)\n",
    "\n",
    "        except:\n",
    "            eval_hit = True\n",
    "            break\n",
    "    if (eval_hit):\n",
    "        fixed_name_list = name_fixer(r_name_list)\n",
    "        name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "    layer_types = name_type_shape[:, 1]\n",
    "    layer_shapes = name_type_shape[:, 2]\n",
    "    mapped_layers = {'model_layer': [], 'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                     'Linear_BatchNorm1d': []}\n",
    "\n",
    "    def detect_sequences(lst):\n",
    "        i = 0\n",
    "        while i < len(lst):\n",
    "\n",
    "            if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                    )\n",
    "                    i += 3\n",
    "\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "            # if i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[0], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            # elif i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[1], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                mapped_layers['Linear_ReLU'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[4],\n",
    "            ]:\n",
    "                mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    detect_sequences(layer_types)\n",
    "\n",
    "    for keys, value in mapped_layers.items():\n",
    "        mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "\n",
    "    mapped_layers['name_type_shape'] = name_type_shape\n",
    "    # self.mapped_layers = mapped_layers\n",
    "\n",
    "    # CWP\n",
    "    keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "    pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "    # CWP or QAT Fusion Layers\n",
    "    for keys in keys_to_lookout:\n",
    "        data = mapped_layers[keys]\n",
    "        if (len(data) != 0):\n",
    "            qat_layer_of_interest.append(data)\n",
    "    mapped_layers['qat_layers'] = np.asarray(qat_layer_of_interest)\n",
    "\n",
    "    return mapped_layers\n",
    "\n",
    "\n",
    "# GMP\n",
    "#         layer_of_interest=mapped_layers['name_type_shape'][:,0] # all layers with weights\n",
    "#         Check for all with weights\n",
    "# Wanda\n",
    "\n",
    "def string_fixer(name_list):\n",
    "    for ind in range(len(name_list)):\n",
    "        modified_string = re.sub(r'\\.(\\[)', r'\\1', name_list[ind])\n",
    "        name_list[ind] = modified_string\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ab0ece",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "def cwp_possible_layers(layer_name_list):\n",
    "    possible_indices = []\n",
    "    idx = 0\n",
    "    \n",
    "    while idx < len(layer_name_list):\n",
    "        current_value = layer_name_list[idx]\n",
    "        layer_shape = eval(current_value).weight.shape\n",
    "        curr_merge_list = []\n",
    "        curr_merge_list.append([current_value, 0])\n",
    "        hit_catch = False\n",
    "        for internal_idx in range(idx + 1, len(layer_name_list) - 1):\n",
    "            new_layer = layer_name_list[internal_idx]\n",
    "            new_layer_shape = eval(new_layer).weight.shape\n",
    "            if len(new_layer_shape) == 4:\n",
    "                curr_merge_list.append([new_layer, 0])\n",
    "                if layer_shape[0] == new_layer_shape[1]:\n",
    "                    hit_catch = True\n",
    "                    break\n",
    "            elif len(new_layer_shape) == 1:\n",
    "                curr_merge_list[len(curr_merge_list) - 1][1] = new_layer\n",
    "        possible_indices.append(curr_merge_list)\n",
    "        if hit_catch == True:\n",
    "            idx = internal_idx\n",
    "        else:\n",
    "            idx += 1\n",
    "    return possible_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7585e32a",
   "metadata": {},
   "source": [
    "\n",
    "# #load the pretrained model\n",
    "\n",
    "# resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# densenet = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "# super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True)\n",
    "\n",
    "super_net_name = \"ofa_supernet_mbv3_w10\" \n",
    "\n",
    "vgg = VGG()\n",
    "mobilenet_v2 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "\n",
    "model_list = [vgg, mobilenet_v2, mobilenet_v3 ]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21540da2",
   "metadata": {},
   "source": [
    "# model = mobilenet_v3\n",
    "\n",
    "for model in model_list:\n",
    "    \n",
    "    mapped_layers = layer_mapping(model)\n",
    "    name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "\n",
    "    possible_indices_ranges = cwp_possible_layers(name_list)\n",
    "    possible_indices_ranges = [lst for lst in possible_indices_ranges if len(lst) > 1]\n",
    "    \n",
    "    pruning_ratio = 0.90\n",
    "    pruning_layer_length = len(possible_indices_ranges)\n",
    "    pruning_ratio_list = (pruning_layer_length) * [pruning_ratio]\n",
    "    \n",
    "    pruned_model, original_model = prune_cwp(model,pruning_ratio_list)\n",
    "    summary(original_model,(3,32,32))\n",
    "    summary(pruned_model,(3,32,32))\n",
    "    \n",
    "    \n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
