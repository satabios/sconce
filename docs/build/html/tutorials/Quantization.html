<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantization &mdash; sconce 0.57 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/my_theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/default.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2fd344d2"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sconce Pipeline Tutorial" href="Pipeline-Tutorial.html" />
    <link rel="prev" title="Pruning" href="Pruning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            sconce
              <img src="../_static/sconce-punch-bk_removed.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.57.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Pruning.html">Pruning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#goals">Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setup">Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lets-first-evaluate-the-accuracy-and-model-size-of-the-fp32-model">Let’s First Evaluate the Accuracy and Model Size of the FP32 Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#k-means-quantization">K-Means Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#k-means-quantization-on-whole-model">K-Means Quantization on Whole Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#trained-k-means-quantization">Trained K-Means Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-quantization">Linear Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#n-bit-integer"><em>n</em>-bit Integer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scale">Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="#zero-point">zero point</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#special-case-linear-quantization-on-weight-tensor">Special case: linear quantization on weight tensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#per-channel-linear-quantization">Per-channel Linear Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-quick-peek-at-linear-quantization-on-weights">A Quick Peek at Linear Quantization on Weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#quantized-inference">Quantized Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quantized-fully-connected-layer">Quantized Fully-Connected Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantized-convolution">Quantized Convolution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Pipeline-Tutorial.html">sconce Pipeline Tutorial</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sconce</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/Quantization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading"></a></h1>
<section id="goals">
<h2>Goals<a class="headerlink" href="#goals" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Understand the basic concept of <strong>quantization</strong></p></li>
<li><p>Implement and apply <strong>k-means quantization</strong></p></li>
<li><p>Implement and apply <strong>quantization-aware training</strong> for k-means
quantization</p></li>
<li><p>Implement and apply <strong>linear quantization</strong></p></li>
<li><p>Implement and apply <strong>integer-only inference</strong> for linear
quantization</p></li>
<li><p>Get a basic understanding of performance improvement (such as
speedup) from quantization</p></li>
<li><p>Understand the differences and tradeoffs between these quantization
approaches</p></li>
</ul>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h2>
<p>First, install the required packages and download the datasets and
pretrained model. Here we use CIFAR10 dataset and VGG network which is
the same as what we used in the Lab 0 tutorial.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Installing torchprofile...&#39;</span><span class="p">)</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>torchprofile<span class="w"> </span><span class="m">1</span>&gt;/dev/null
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Installing fast-pytorch-kmeans...&#39;</span><span class="p">)</span>
<span class="o">!</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>fast-pytorch-kmeans<span class="w"> </span><span class="m">1</span>&gt;/dev/null
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All required packages have been successfully installed!&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Installing torchprofile...
Installing fast-pytorch-kmeans...
All required packages have been successfully installed!
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchprofile</span> <span class="kn">import</span> <span class="n">profile_macs</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">from</span> <span class="nn">torchprofile</span> <span class="kn">import</span> <span class="n">profile_macs</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">(),</span> \
<span class="s2">&quot;The current runtime does not have CUDA support.&quot;</span> \
<span class="s2">&quot;Please go to menu bar (Runtime - Change runtime type) and select GPU&quot;</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">Generator</span> <span class="n">at</span> <span class="mh">0x7f51fd7c5810</span><span class="o">&gt;</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">download_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">sys</span>
    <span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
    <span class="n">target_dir</span> <span class="o">=</span> <span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">model_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
        <span class="n">model_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">target_dir</span><span class="p">)</span>
        <span class="n">cached_file</span> <span class="o">=</span> <span class="n">model_dir</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cached_file</span><span class="p">)</span> <span class="ow">or</span> <span class="n">overwrite</span><span class="p">:</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;Downloading: &quot;</span><span class="si">{}</span><span class="s1">&quot; to </span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">cached_file</span><span class="p">))</span>
            <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">cached_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cached_file</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># remove lock file so download can be executed next time.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="s1">&#39;download.lock&#39;</span><span class="p">))</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;Failed to download from url </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">url</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="n">ARCH</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}{</span><span class="n">counts</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">layer</span><span class="p">))</span>
      <span class="n">counts</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ARCH</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="s1">&#39;M&#39;</span><span class="p">:</span>
        <span class="c1"># conv-bn-relu</span>
        <span class="n">add</span><span class="p">(</span><span class="s2">&quot;conv&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="n">add</span><span class="p">(</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">add</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">in_channels</span> <span class="o">=</span> <span class="n">x</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># maxpool</span>
        <span class="n">add</span><span class="p">(</span><span class="s2">&quot;pool&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">add</span><span class="p">(</span><span class="s2">&quot;avgpool&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">(</span><span class="n">layers</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># avgpool: [N, 512, 2, 2] =&gt; [N, 512]</span>
    <span class="c1"># x = x.mean([2, 3])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># classifier: [N, 512] =&gt; [N, 10]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
  <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
  <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
  <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
  <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
  <span class="n">scheduler</span><span class="p">:</span> <span class="n">LambdaLR</span><span class="p">,</span>
  <span class="n">callbacks</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Move the data from CPU to GPU</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># Reset the gradients (from the last iteration)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Forward inference</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Backward propagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update optimizer and LR scheduler</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="n">callbacks</span><span class="p">:</span>
            <span class="n">callback</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
  <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
  <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
  <span class="n">extra_preprocess</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

  <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Move the data from CPU to GPU</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">extra_preprocess</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">preprocess</span> <span class="ow">in</span> <span class="n">extra_preprocess</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># Inference</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Convert logits to class indices</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Update metrics</span>
    <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">num_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

  <span class="k">return</span> <span class="p">(</span><span class="n">num_correct</span> <span class="o">/</span> <span class="n">num_samples</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
<p>Helpler Functions (Flops, Model Size calculation, etc.)</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model_flops</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">num_macs</span> <span class="o">=</span> <span class="n">profile_macs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">num_macs</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">data_width</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    calculate the model size in bits</span>
<span class="sd">    :param data_width: #bits per element</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_elements</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">num_elements</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">num_elements</span> <span class="o">*</span> <span class="n">data_width</span>

<span class="n">Byte</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">KiB</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="n">Byte</span>
<span class="n">MiB</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="n">KiB</span>
<span class="n">GiB</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="n">MiB</span>
</pre></div>
</div>
<p>Define misc funcions for verification.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_k_means_quantize</span><span class="p">(</span>
    <span class="n">test_tensor</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3747</span><span class="p">,</span>  <span class="mf">0.0874</span><span class="p">,</span>  <span class="mf">0.3200</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4868</span><span class="p">,</span>  <span class="mf">0.4404</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0402</span><span class="p">,</span>  <span class="mf">0.2322</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2024</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4986</span><span class="p">,</span>  <span class="mf">0.1814</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3102</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3942</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2030</span><span class="p">,</span>  <span class="mf">0.0883</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4741</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1592</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0777</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3946</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2128</span><span class="p">,</span>  <span class="mf">0.2675</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0611</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1933</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4350</span><span class="p">,</span>  <span class="mf">0.2928</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1087</span><span class="p">]]),</span>
    <span class="n">bitwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">plot_matrix</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;white&#39;</span><span class="p">])):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                                <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    <span class="n">ax_left</span><span class="p">,</span> <span class="n">ax_right</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">,</span> <span class="n">ax_left</span><span class="p">,</span> <span class="s1">&#39;original tensor&#39;</span><span class="p">)</span>

    <span class="n">num_unique_values_before_quantization</span> <span class="o">=</span> <span class="n">test_tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">k_means_quantize</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">num_unique_values_after_quantization</span> <span class="o">=</span> <span class="n">test_tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test k_means_quantize()&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;    target bitwidth: </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        num unique values before k-means quantization: </span><span class="si">{</span><span class="n">num_unique_values_before_quantization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        num unique values after  k-means quantization: </span><span class="si">{</span><span class="n">num_unique_values_after_quantization</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">num_unique_values_after_quantization</span> <span class="o">==</span> <span class="nb">min</span><span class="p">((</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">bitwidth</span><span class="p">),</span> <span class="n">num_unique_values_before_quantization</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test passed.&#39;</span><span class="p">)</span>

    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">,</span> <span class="n">ax_right</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1">-bit k-means quantized tensor&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_linear_quantize</span><span class="p">(</span>
    <span class="n">test_tensor</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span> <span class="mf">0.0523</span><span class="p">,</span>  <span class="mf">0.6364</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0968</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0020</span><span class="p">,</span>  <span class="mf">0.1940</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.7500</span><span class="p">,</span>  <span class="mf">0.5507</span><span class="p">,</span>  <span class="mf">0.6188</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1734</span><span class="p">,</span>  <span class="mf">0.4677</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0669</span><span class="p">,</span>  <span class="mf">0.3836</span><span class="p">,</span>  <span class="mf">0.4297</span><span class="p">,</span>  <span class="mf">0.6267</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0695</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1536</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0038</span><span class="p">,</span>  <span class="mf">0.6075</span><span class="p">,</span>  <span class="mf">0.6817</span><span class="p">,</span>  <span class="mf">0.0601</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6446</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2500</span><span class="p">,</span>  <span class="mf">0.5376</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2226</span><span class="p">,</span>  <span class="mf">0.2333</span><span class="p">]]),</span>
    <span class="n">quantized_test_tensor</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
    <span class="n">real_min</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">real_max</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">plot_matrix</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;white&#39;</span><span class="p">])):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">datum</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">datum</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
                    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">datum</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                                    <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">datum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                                    <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
    <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;original tensor&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">real_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">real_max</span><span class="p">)</span>
    <span class="n">_quantized_test_tensor</span> <span class="o">=</span> <span class="n">linear_quantize</span><span class="p">(</span>
        <span class="n">test_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="n">bitwidth</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=</span><span class="n">zero_point</span><span class="p">)</span>
    <span class="n">_reconstructed_test_tensor</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">_quantized_test_tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="n">zero_point</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test linear_quantize()&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;    target bitwidth: </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        scale: </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        zero point: </span><span class="si">{</span><span class="n">zero_point</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">_quantized_test_tensor</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">quantized_test_tensor</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test passed.&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">_quantized_test_tensor</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;2-bit linear quantized tensor&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">quantized_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">_reconstructed_test_tensor</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;reconstructed tensor&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="n">real_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">real_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_quantized_fc</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span><span class="mf">0.6118</span><span class="p">,</span> <span class="mf">0.7288</span><span class="p">,</span> <span class="mf">0.8511</span><span class="p">,</span> <span class="mf">0.2849</span><span class="p">,</span> <span class="mf">0.8427</span><span class="p">,</span> <span class="mf">0.7435</span><span class="p">,</span> <span class="mf">0.4014</span><span class="p">,</span> <span class="mf">0.2794</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3676</span><span class="p">,</span> <span class="mf">0.2426</span><span class="p">,</span> <span class="mf">0.1612</span><span class="p">,</span> <span class="mf">0.7684</span><span class="p">,</span> <span class="mf">0.6038</span><span class="p">,</span> <span class="mf">0.0400</span><span class="p">,</span> <span class="mf">0.2240</span><span class="p">,</span> <span class="mf">0.4237</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6565</span><span class="p">,</span> <span class="mf">0.6878</span><span class="p">,</span> <span class="mf">0.4670</span><span class="p">,</span> <span class="mf">0.3470</span><span class="p">,</span> <span class="mf">0.2281</span><span class="p">,</span> <span class="mf">0.8074</span><span class="p">,</span> <span class="mf">0.0178</span><span class="p">,</span> <span class="mf">0.3999</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1863</span><span class="p">,</span> <span class="mf">0.3567</span><span class="p">,</span> <span class="mf">0.6104</span><span class="p">,</span> <span class="mf">0.0497</span><span class="p">,</span> <span class="mf">0.0577</span><span class="p">,</span> <span class="mf">0.2990</span><span class="p">,</span> <span class="mf">0.6687</span><span class="p">,</span> <span class="mf">0.8626</span><span class="p">]]),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span> <span class="mf">1.2626e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4752e-01</span><span class="p">,</span>  <span class="mf">8.1910e-02</span><span class="p">,</span>  <span class="mf">2.4982e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0495e-01</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.9227e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8550e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5700e-01</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.7624e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3835e-01</span><span class="p">,</span>  <span class="mf">5.1010e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2020e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0344e-01</span><span class="p">,</span>
          <span class="mf">1.0202e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0799e-01</span><span class="p">,</span>  <span class="mf">2.4112e-01</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">3.8216e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8047e-01</span><span class="p">,</span>  <span class="mf">8.5238e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2504e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0952e-01</span><span class="p">,</span>
          <span class="mf">3.2018e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3619e-01</span><span class="p">,</span>  <span class="mf">2.0219e-01</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.9233e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0124e-01</span><span class="p">,</span>  <span class="mf">1.1467e-01</span><span class="p">,</span>  <span class="mf">2.0091e-01</span><span class="p">,</span>  <span class="mf">1.1438e-01</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">4.2427e-01</span><span class="p">,</span>  <span class="mf">1.0178e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0941e-04</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.8837e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1256e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5285e-01</span><span class="p">,</span>  <span class="mf">2.0949e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8684e-01</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.7100e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5331e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0433e-01</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.0038e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3757e-02</span><span class="p">,</span>  <span class="mf">1.8997e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.6866e-01</span><span class="p">,</span>  <span class="mf">5.5484e-02</span><span class="p">,</span>
          <span class="mf">1.5643e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3538e-01</span><span class="p">,</span>  <span class="mf">2.1103e-01</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.6875e-01</span><span class="p">,</span>  <span class="mf">2.4984e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3514e-01</span><span class="p">,</span>  <span class="mf">2.5527e-01</span><span class="p">,</span>  <span class="mf">2.0322e-01</span><span class="p">,</span>
          <span class="mf">3.7675e-01</span><span class="p">,</span>  <span class="mf">6.1563e-02</span><span class="p">,</span>  <span class="mf">1.7201e-01</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.3541e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3555e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3349e-01</span><span class="p">,</span>  <span class="mf">4.3043e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0498e-01</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.8366e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.1553e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1168e-01</span><span class="p">]]),</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.1954</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2756</span><span class="p">,</span>  <span class="mf">0.3113</span><span class="p">,</span>  <span class="mf">0.1149</span><span class="p">,</span>  <span class="mf">0.4274</span><span class="p">,</span>  <span class="mf">0.2429</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1721</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2502</span><span class="p">]),</span>
    <span class="n">quantized_bias</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="n">shifted_quantized_bias</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="n">calc_quantized_output</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
    <span class="n">bitwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">plot_matrix</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;white&#39;</span><span class="p">])):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">datum</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">datum</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
                    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">datum</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                                    <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">datum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                                    <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">weight_zero_point</span> <span class="o">=</span> \
        <span class="n">linear_quantize_weight_per_channel</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">quantized_input</span><span class="p">,</span> <span class="n">input_scale</span><span class="p">,</span> <span class="n">input_zero_point</span> <span class="o">=</span> \
        <span class="n">linear_quantize_feature</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">_quantized_bias</span><span class="p">,</span> <span class="n">bias_scale</span><span class="p">,</span> <span class="n">bias_zero_point</span> <span class="o">=</span> \
        <span class="n">linear_quantize_bias_per_output_channel</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">input_scale</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">_quantized_bias</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">_quantized_bias</span><span class="p">)</span>
    <span class="n">_shifted_quantized_bias</span> <span class="o">=</span> \
        <span class="n">shift_quantized_linear_bias</span><span class="p">(</span><span class="n">quantized_bias</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_zero_point</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">_shifted_quantized_bias</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">shifted_quantized_bias</span><span class="p">)</span>
    <span class="n">quantized_output</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span> <span class="n">output_zero_point</span> <span class="o">=</span> \
        <span class="n">linear_quantize_feature</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>

    <span class="n">_calc_quantized_output</span> <span class="o">=</span> <span class="n">quantized_linear</span><span class="p">(</span>
        <span class="n">quantized_input</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">shifted_quantized_bias</span><span class="p">,</span>
        <span class="n">bitwidth</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">,</span>
        <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
        <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">_calc_quantized_output</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">calc_quantized_output</span><span class="p">)</span>

    <span class="n">reconstructed_weight</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">quantized_weight</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="n">weight_zero_point</span><span class="p">)</span>
    <span class="n">reconstructed_input</span> <span class="o">=</span> <span class="n">input_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">quantized_input</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="n">input_zero_point</span><span class="p">)</span>
    <span class="n">reconstructed_bias</span> <span class="o">=</span> <span class="n">bias_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">quantized_bias</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="n">bias_zero_point</span><span class="p">)</span>
    <span class="n">reconstructed_calc_output</span> <span class="o">=</span> <span class="n">output_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">calc_quantized_output</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="n">output_zero_point</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;original weight&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;original input&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;original output&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1">-bit linear quantized weight&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">quantized_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">quantized_input</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1">-bit linear quantized input&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">quantized_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">calc_quantized_output</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantized output from quantized_linear()&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">quantized_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">reconstructed_weight</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;reconstructed weight&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">reconstructed_input</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;reconstructed input&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>
    <span class="n">plot_matrix</span><span class="p">(</span><span class="n">reconstructed_calc_output</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;reconstructed output&#39;</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20c&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test quantized_fc()&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;    target bitwidth: </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;      batch size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;      input channels: </span><span class="si">{</span><span class="n">in_channels</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;      output channels: </span><span class="si">{</span><span class="n">out_channels</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;* Test passed.&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Load Pretrained Model</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint_url</span> <span class="o">=</span> <span class="s2">&quot;https://hanlab.mit.edu/files/course/labs/vgg.cifar.pretrained.pth&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">download_url</span><span class="p">(</span><span class="n">checkpoint_url</span><span class="p">),</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VGG</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;=&gt; loading checkpoint &#39;</span><span class="si">{</span><span class="n">checkpoint_url</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">])</span>
<span class="n">recover_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">=&gt;</span> <span class="n">loading</span> <span class="n">checkpoint</span> <span class="s1">&#39;https://hanlab.mit.edu/files/course/labs/vgg.cifar.pretrained.pth&#39;</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">transforms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">Compose</span><span class="p">([</span>
        <span class="n">RandomCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">ToTensor</span><span class="p">(),</span>
    <span class="p">]),</span>
    <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">}</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">]:</span>
  <span class="n">dataset</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/cifar10&quot;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="p">(</span><span class="n">split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">[</span><span class="n">split</span><span class="p">],</span>
  <span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]:</span>
  <span class="n">dataloader</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">split</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">split</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">),</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Files</span> <span class="n">already</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">verified</span>
<span class="n">Files</span> <span class="n">already</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">verified</span>
</pre></div>
</div>
</section>
<section id="lets-first-evaluate-the-accuracy-and-model-size-of-the-fp32-model">
<h2>Let’s First Evaluate the Accuracy and Model Size of the FP32 Model<a class="headerlink" href="#lets-first-evaluate-the-accuracy-and-model-size-of-the-fp32-model" title="Link to this heading"></a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fp32_model_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
<span class="n">fp32_model_size</span> <span class="o">=</span> <span class="n">get_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;fp32 model has accuracy=</span><span class="si">{</span><span class="n">fp32_model_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;fp32 model has size=</span><span class="si">{</span><span class="n">fp32_model_size</span><span class="o">/</span><span class="n">MiB</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MiB&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fp32</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">92.95</span><span class="o">%</span>
<span class="n">fp32</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">35.20</span> <span class="n">MiB</span>
</pre></div>
</div>
</section>
<section id="k-means-quantization">
<h2>K-Means Quantization<a class="headerlink" href="#k-means-quantization" title="Link to this heading"></a></h2>
<p>Network quantization compresses the network by reducing the bits per
weight required to represent the deep network. The quantized network can
have a faster inference speed with hardware support.</p>
<p>In this section, we will explore the K-means quantization for neural
networks as in <a class="reference external" href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks With
Pruning, Trained Quantization And Huffman
Coding</a>.</p>
<figure class="align-default" id="id1">
<img alt="kmeans.png" src="data:image/png;base64,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" />
<figcaption>
<p><span class="caption-text">kmeans.png</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>A <span class="math notranslate nohighlight">\(n\)</span>-bit k-means quantization will divide synapses into
<span class="math notranslate nohighlight">\(2^n\)</span> clusters, and synapses in the same cluster will share the
same weight value.</p>
<p>Therefore, k-means quantization will create a codebook, inlcuding *
<code class="docutils literal notranslate"><span class="pre">centroids</span></code>: <span class="math notranslate nohighlight">\(2^n\)</span> fp32 cluster centers. * <code class="docutils literal notranslate"><span class="pre">labels</span></code>: a
<span class="math notranslate nohighlight">\(n\)</span>-bit integer tensor with the same #elements of the original
fp32 weights tensor. Each integer indicates which cluster it belongs to.</p>
<p>During the inference, a fp32 tensor is generated based on the codebook
for inference:</p>
<blockquote>
<div><p><strong>quantized_weight =
codebook.centroids[codebook.labels].view_as(weight)</strong></p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>

<span class="n">Codebook</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Codebook&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;centroids&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fast_pytorch_kmeans</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="k">def</span> <span class="nf">k_means_quantize</span><span class="p">(</span><span class="n">fp32_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">codebook</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    quantize tensor using k-means clustering</span>
<span class="sd">    :param fp32_tensor:</span>
<span class="sd">    :param bitwidth: [int] quantization bit width, default=4</span>
<span class="sd">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span>
<span class="sd">    :return:</span>
<span class="sd">        [Codebook = (centroids, labels)]</span>
<span class="sd">            centroids: [torch.(cuda.)FloatTensor] the cluster centroids</span>
<span class="sd">            labels: [torch.(cuda.)LongTensor] cluster label tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">codebook</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

        <span class="c1"># get number of clusters based on the quantization precision</span>
        <span class="c1"># hint: one line of code</span>
        <span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">bitwidth</span>

        <span class="c1"># use k-means to get the quantization centroids</span>
        <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">fp32_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">centroids</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">centroids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">codebook</span> <span class="o">=</span> <span class="n">Codebook</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="c1"># decode the codebook into k-means quantized tensor for inference</span>
    <span class="c1"># hint: one line of code</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">codebook</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">codebook</span><span class="o">.</span><span class="n">labels</span><span class="p">]</span>

    <span class="n">fp32_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">quantized_tensor</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">fp32_tensor</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">codebook</span>
</pre></div>
</div>
<p>Let’s verify the functionality of defined k-means quantization by
applying the function above on a dummy tensor.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_k_means_quantize</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*</span> <span class="n">Test</span> <span class="n">k_means_quantize</span><span class="p">()</span>
    <span class="n">target</span> <span class="n">bitwidth</span><span class="p">:</span> <span class="mi">2</span> <span class="n">bits</span>
        <span class="n">num</span> <span class="n">unique</span> <span class="n">values</span> <span class="n">before</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantization</span><span class="p">:</span> <span class="mi">25</span>
        <span class="n">num</span> <span class="n">unique</span> <span class="n">values</span> <span class="n">after</span>  <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantization</span><span class="p">:</span> <span class="mi">4</span>
<span class="o">*</span> <span class="n">Test</span> <span class="n">passed</span><span class="o">.</span>
</pre></div>
</div>
<img alt="../_images/Quantization_30_1.png" src="../_images/Quantization_30_1.png" />
<p>The last code cell performs 2-bit k-means quantization and plots the
tensor before and after the quantization. Each cluster is rendered with
a unique color. There are 4 unique colors rendered in the quantized
tensor.</p>
</section>
<section id="k-means-quantization-on-whole-model">
<h2>K-Means Quantization on Whole Model<a class="headerlink" href="#k-means-quantization-on-whole-model" title="Link to this heading"></a></h2>
<p>Similar to what we did in lab 1, we now wrap the k-means quantization
function into a class for quantizing the whole model. In class
<code class="docutils literal notranslate"><span class="pre">KMeansQuantizer</span></code>, we have to keep a record of the codebooks (i.e.,
<code class="docutils literal notranslate"><span class="pre">centroids</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code>) so that we could apply or update the
codebooks whenever the model weights change.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">parameter</span>
<span class="k">class</span> <span class="nc">KMeansQuantizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span> <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">codebook</span> <span class="o">=</span> <span class="n">KMeansQuantizer</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">update_centroids</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">codebook</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">update_centroids</span><span class="p">:</span>
                    <span class="n">update_codebook</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">codebook</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">codebook</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">codebook</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_means_quantize</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">codebook</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">codebook</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">codebook</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">bitwidth</span><span class="p">:</span>
                    <span class="n">codebook</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_means_quantize</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="n">bitwidth</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">codebook</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_means_quantize</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="n">bitwidth</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">codebook</span>
</pre></div>
</div>
<p>Now let’s quantize model into 8 bits, 4 bits and 2 bits using K-Means
Quantization. <em>Note that we ignore the storage for codebooks when
calculating the model size.</em></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Note that the storage for codebooks is ignored when calculating the model size.&#39;</span><span class="p">)</span>
<span class="n">quantizers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">bitwidth</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">recover_model</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;k-means quantizing model into </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits&#39;</span><span class="p">)</span>
    <span class="n">quantizer</span> <span class="o">=</span> <span class="n">KMeansQuantizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">quantized_model_size</span> <span class="o">=</span> <span class="n">get_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">-bit k-means quantized model has size=</span><span class="si">{</span><span class="n">quantized_model_size</span><span class="o">/</span><span class="n">MiB</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MiB&quot;</span><span class="p">)</span>
    <span class="n">quantized_model_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">-bit k-means quantized model has accuracy=</span><span class="si">{</span><span class="n">quantized_model_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="n">quantizers</span><span class="p">[</span><span class="n">bitwidth</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Note</span> <span class="n">that</span> <span class="n">the</span> <span class="n">storage</span> <span class="k">for</span> <span class="n">codebooks</span> <span class="ow">is</span> <span class="n">ignored</span> <span class="n">when</span> <span class="n">calculating</span> <span class="n">the</span> <span class="n">model</span> <span class="n">size</span><span class="o">.</span>
<span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">8</span> <span class="n">bits</span>
    <span class="mi">8</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">8.80</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="mi">8</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">92.78</span><span class="o">%</span>
<span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">4</span> <span class="n">bits</span>
    <span class="mi">4</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">4.40</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="mi">4</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">79.08</span><span class="o">%</span>
<span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">2</span> <span class="n">bits</span>
    <span class="mi">2</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">2.20</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">10.00</span><span class="o">%</span>
</pre></div>
</div>
</section>
<section id="trained-k-means-quantization">
<h2>Trained K-Means Quantization<a class="headerlink" href="#trained-k-means-quantization" title="Link to this heading"></a></h2>
<p>As we can see from the results of last cell, the accuracy significantly
drops when quantizing the model into lower bits. Therefore, we have to
perform quantization-aware training to recover the accuracy.</p>
<p>During the k-means quantization-aware training, the centroids are also
updated, which is proposed in <a class="reference external" href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural
Networks With Pruning, Trained Quantization And Huffman
Coding</a>.</p>
<p>The gradient for the centroids is calculated as,</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \frac{\partial W_{j} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \mathbf{1}(I_{j}=k)\)</span></p>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is the loss, <span class="math notranslate nohighlight">\(C_k\)</span> is <em>k</em>-th centroid,
<span class="math notranslate nohighlight">\(I_{j}\)</span> is the label for weight <span class="math notranslate nohighlight">\(W_{j}\)</span>.
<span class="math notranslate nohighlight">\(\mathbf{1}()\)</span> is the indicator function, and
<span class="math notranslate nohighlight">\(\mathbf{1}(I_{j}=k)\)</span> means
<span class="math notranslate nohighlight">\(1\;\mathrm{if}\;I_{j}=k\;\mathrm{else}\;0\)</span>, <em>i.e.</em>,
<span class="math notranslate nohighlight">\(I_{j}==k\)</span>.</p>
<p>Here in the lab, <strong>for simplicity</strong>, we directly update the centroids
according to the latest weights:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(C_k = \frac{\sum_{j}W_{j}\mathbf{1}(I_{j}=k)}{\sum_{j}\mathbf{1}(I_{j}=k)}\)</span></p>
</div></blockquote>
<p>The above equation for updating centroids is indeed using the <code class="docutils literal notranslate"><span class="pre">mean</span></code>
of weights in the same cluster to be the updated centroid value.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_codebook</span><span class="p">(</span><span class="n">fp32_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">codebook</span><span class="p">:</span> <span class="n">Codebook</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    update the centroids in the codebook using updated fp32_tensor</span>
<span class="sd">    :param fp32_tensor: [torch.(cuda.)Tensor]</span>
<span class="sd">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_clusters</span> <span class="o">=</span> <span class="n">codebook</span><span class="o">.</span><span class="n">centroids</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">fp32_tensor</span> <span class="o">=</span> <span class="n">fp32_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>

        <span class="c1"># hint: one line of code</span>
        <span class="n">codebook</span><span class="o">.</span><span class="n">centroids</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fp32_tensor</span><span class="p">[</span><span class="n">codebook</span><span class="o">.</span><span class="n">labels</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>Now let’s run the following code cell to finetune the k-means quantized
model to recover the accuracy. We will stop finetuning if accuracy drop
is less than 0.5.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_drop_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">quantizers_before_finetune</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">quantizers</span><span class="p">)</span>
<span class="n">quantizers_after_finetune</span> <span class="o">=</span> <span class="n">quantizers</span>

<span class="k">for</span> <span class="n">bitwidth</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">recover_model</span><span class="p">()</span>
    <span class="n">quantizer</span> <span class="o">=</span> <span class="n">quantizers</span><span class="p">[</span><span class="n">bitwidth</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;k-means quantizing model into </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits&#39;</span><span class="p">)</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">update_centroids</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">quantized_model_size</span> <span class="o">=</span> <span class="n">get_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">-bit k-means quantized model has size=</span><span class="si">{</span><span class="n">quantized_model_size</span><span class="o">/</span><span class="n">MiB</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MiB&quot;</span><span class="p">)</span>
    <span class="n">quantized_model_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">-bit k-means quantized model has accuracy=</span><span class="si">{</span><span class="n">quantized_model_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% before quantization-aware training &quot;</span><span class="p">)</span>
    <span class="n">accuracy_drop</span> <span class="o">=</span> <span class="n">fp32_model_accuracy</span> <span class="o">-</span> <span class="n">quantized_model_accuracy</span>
    <span class="k">if</span> <span class="n">accuracy_drop</span> <span class="o">&gt;</span> <span class="n">accuracy_drop_threshold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;        Quantization-aware training due to accuracy drop=</span><span class="si">{</span><span class="n">accuracy_drop</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% is larger than threshold=</span><span class="si">{</span><span class="n">accuracy_drop_threshold</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="n">num_finetune_epochs</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_finetune_epochs</span><span class="p">)</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="n">num_finetune_epochs</span>
        <span class="k">while</span> <span class="n">accuracy_drop</span> <span class="o">&gt;</span> <span class="n">accuracy_drop_threshold</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span>
                  <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="k">lambda</span><span class="p">:</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">update_centroids</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
            <span class="n">model_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
            <span class="n">is_best</span> <span class="o">=</span> <span class="n">model_accuracy</span> <span class="o">&gt;</span> <span class="n">best_accuracy</span>
            <span class="n">best_accuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">model_accuracy</span><span class="p">,</span> <span class="n">best_accuracy</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        Epoch </span><span class="si">{</span><span class="n">num_finetune_epochs</span><span class="o">-</span><span class="n">epoch</span><span class="si">}</span><span class="s1"> Accuracy </span><span class="si">{</span><span class="n">model_accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">% / Best Accuracy: </span><span class="si">{</span><span class="n">best_accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
            <span class="n">accuracy_drop</span> <span class="o">=</span> <span class="n">fp32_model_accuracy</span> <span class="o">-</span> <span class="n">best_accuracy</span>
            <span class="n">epoch</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;        No need for quantization-aware training since accuracy drop=</span><span class="si">{</span><span class="n">accuracy_drop</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% is smaller than threshold=</span><span class="si">{</span><span class="n">accuracy_drop_threshold</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">8</span> <span class="n">bits</span>
    <span class="mi">8</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">8.80</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="mi">8</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">92.78</span><span class="o">%</span> <span class="n">before</span> <span class="n">quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span>
        <span class="n">No</span> <span class="n">need</span> <span class="k">for</span> <span class="n">quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span> <span class="n">since</span> <span class="n">accuracy</span> <span class="n">drop</span><span class="o">=</span><span class="mf">0.17</span><span class="o">%</span> <span class="ow">is</span> <span class="n">smaller</span> <span class="n">than</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.50</span><span class="o">%</span>
<span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">4</span> <span class="n">bits</span>
    <span class="mi">4</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">4.40</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">4</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">79.08</span><span class="o">%</span> <span class="n">before</span> <span class="n">quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span>
    <span class="n">Quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span> <span class="n">due</span> <span class="n">to</span> <span class="n">accuracy</span> <span class="n">drop</span><span class="o">=</span><span class="mf">13.87</span><span class="o">%</span> <span class="ow">is</span> <span class="n">larger</span> <span class="n">than</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.50</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">Epoch</span> <span class="mi">0</span> <span class="n">Accuracy</span> <span class="mf">92.48</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">92.48</span><span class="o">%</span>
<span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantizing</span> <span class="n">model</span> <span class="n">into</span> <span class="mi">2</span> <span class="n">bits</span>
    <span class="mi">2</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">size</span><span class="o">=</span><span class="mf">2.20</span> <span class="n">MiB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span><span class="o">-</span><span class="n">bit</span> <span class="n">k</span><span class="o">-</span><span class="n">means</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">10.00</span><span class="o">%</span> <span class="n">before</span> <span class="n">quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span>
    <span class="n">Quantization</span><span class="o">-</span><span class="n">aware</span> <span class="n">training</span> <span class="n">due</span> <span class="n">to</span> <span class="n">accuracy</span> <span class="n">drop</span><span class="o">=</span><span class="mf">82.95</span><span class="o">%</span> <span class="ow">is</span> <span class="n">larger</span> <span class="n">than</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.50</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span> <span class="n">Accuracy</span> <span class="mf">90.16</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">90.16</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">1</span> <span class="n">Accuracy</span> <span class="mf">90.82</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">90.82</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">2</span> <span class="n">Accuracy</span> <span class="mf">91.04</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">91.04</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">3</span> <span class="n">Accuracy</span> <span class="mf">91.11</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">91.11</span><span class="o">%</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>train:   0%|          | 0/98 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">4</span> <span class="n">Accuracy</span> <span class="mf">91.19</span><span class="o">%</span> <span class="o">/</span> <span class="n">Best</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">91.19</span><span class="o">%</span>
</pre></div>
</div>
</section>
<section id="linear-quantization">
<h2>Linear Quantization<a class="headerlink" href="#linear-quantization" title="Link to this heading"></a></h2>
<p>In this section, we will implement and perform linear quantization.</p>
<p>Linear quantization directly rounds the floating-point value into the
nearest quantized integer after range truncation and scaling.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1712.05877.pdf">Linear quantization</a> can be
represented as</p>
<p><span class="math notranslate nohighlight">\(r = S(q-Z)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(r\)</span> is a floating point real number, <span class="math notranslate nohighlight">\(q\)</span> is a <em>n</em>-bit
integer, <span class="math notranslate nohighlight">\(Z\)</span> is a <em>n</em>-bit integer, and <span class="math notranslate nohighlight">\(S\)</span> is a floating
point real number. <span class="math notranslate nohighlight">\(Z\)</span> is quantization zero point and <span class="math notranslate nohighlight">\(S\)</span> is
quantization scaling factor. Both constant <span class="math notranslate nohighlight">\(Z\)</span> and <span class="math notranslate nohighlight">\(S\)</span> are
quantization parameters.</p>
</section>
<section id="n-bit-integer">
<h2><em>n</em>-bit Integer<a class="headerlink" href="#n-bit-integer" title="Link to this heading"></a></h2>
<p>A <em>n</em>-bit signed integer is usually represented in <a class="reference external" href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s
complement</a>
notation.</p>
<p>A <em>n</em>-bit signed integer can enode integers in the range
<span class="math notranslate nohighlight">\([-2^{n-1}, 2^{n-1}-1]\)</span>. For example, a 8-bit integer falls in the
range [-128, 127].</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">):</span>
    <span class="n">quantized_max</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">bitwidth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">quantized_min</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">bitwidth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span>
</pre></div>
</div>
<ul class="simple">
<li><p>From <span class="math notranslate nohighlight">\(r=S(q-Z)\)</span>, we have <span class="math notranslate nohighlight">\(q = r/S + Z\)</span>.</p></li>
<li><p>Both <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(S\)</span> are floating numbers, and thus we cannot
directly add integer <span class="math notranslate nohighlight">\(Z\)</span> to <span class="math notranslate nohighlight">\(r/S\)</span>. Therefore
<span class="math notranslate nohighlight">\(q = \mathrm{int}(\mathrm{round}(r/S)) + Z\)</span>.</p></li>
<li><p>To convert
<code class="docutils literal notranslate"><span class="pre">`torch.FloatTensor</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>&gt;`__
to
<code class="docutils literal notranslate"><span class="pre">`torch.IntTensor</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>&gt;`__,
we could use
<code class="docutils literal notranslate"><span class="pre">`torch.round()</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round">https://pytorch.org/docs/stable/generated/torch.round.html#torch.round</a>&gt;`__,
<code class="docutils literal notranslate"><span class="pre">`torch.Tensor.round()</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round">https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round</a>&gt;`__,
<code class="docutils literal notranslate"><span class="pre">`torch.Tensor.round_()</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/">https://pytorch.org/docs/stable/generated/</a><a href="#id2"><span class="problematic" id="id3">torch.Tensor.round_</span></a>&gt;`__
to first convert all values to floating integer, and then use
<code class="docutils literal notranslate"><span class="pre">`torch.Tensor.to(torch.int8)</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to">https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to</a>&gt;`__
to convert the data type from
<code class="docutils literal notranslate"><span class="pre">`torch.float</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>&gt;`__ to
<code class="docutils literal notranslate"><span class="pre">`torch.int8</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>&gt;`__.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_quantize</span><span class="p">(</span><span class="n">fp_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    linear quantization for single fp_tensor</span>
<span class="sd">      from</span>
<span class="sd">        fp_tensor = (quantized_tensor - zero_point) * scale</span>
<span class="sd">      we have,</span>
<span class="sd">        quantized_tensor = int(round(fp_tensor / scale)) + zero_point</span>
<span class="sd">    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized</span>
<span class="sd">    :param bitwidth: [int] quantization bit width</span>
<span class="sd">    :param scale: [torch.(cuda.)FloatTensor] scaling factor</span>
<span class="sd">    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">fp_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span>
           <span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="ow">and</span> <span class="n">scale</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">fp_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span>
           <span class="p">(</span><span class="n">zero_point</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">fp_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>


    <span class="c1"># Step 1: scale the fp_tensor</span>
    <span class="n">scaled_tensor</span> <span class="o">=</span> <span class="n">fp_tensor</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
    <span class="c1"># Step 2: round the floating value to integer value</span>
    <span class="n">rounded_tensor</span> <span class="o">=</span> <span class="n">scaled_tensor</span><span class="o">.</span><span class="n">round_</span><span class="p">()</span>


    <span class="n">rounded_tensor</span> <span class="o">=</span> <span class="n">rounded_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>


    <span class="c1"># Step 3: shift the rounded_tensor to make zero_point 0</span>
    <span class="n">shifted_tensor</span> <span class="o">=</span> <span class="n">rounded_tensor</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span>


    <span class="c1"># Step 4: clamp the shifted_tensor to lie in bitwidth-bit range</span>
    <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">shifted_tensor</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_tensor</span>
</pre></div>
</div>
<p>Let’s verify the functionality of defined linear quantization by
applying the function above on a dummy tensor.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_linear_quantize</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*</span> <span class="n">Test</span> <span class="n">linear_quantize</span><span class="p">()</span>
    <span class="n">target</span> <span class="n">bitwidth</span><span class="p">:</span> <span class="mi">2</span> <span class="n">bits</span>
        <span class="n">scale</span><span class="p">:</span> <span class="mf">0.3333333333333333</span>
        <span class="n">zero</span> <span class="n">point</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span>
<span class="o">*</span> <span class="n">Test</span> <span class="n">passed</span><span class="o">.</span>
</pre></div>
</div>
<img alt="../_images/Quantization_48_1.png" src="../_images/Quantization_48_1.png" />
<p>Now we have to determine the scaling factor <span class="math notranslate nohighlight">\(S\)</span> and zero point
<span class="math notranslate nohighlight">\(Z\)</span> for linear quantization.</p>
<p>Recall that <a class="reference external" href="https://arxiv.org/pdf/1712.05877.pdf">linear
quantization</a> can be
represented as</p>
<p><span class="math notranslate nohighlight">\(r = S(q-Z)\)</span></p>
<section id="scale">
<h3>Scale<a class="headerlink" href="#scale" title="Link to this heading"></a></h3>
<p>Linear quantization projects the floating point range [<em>fp_min</em>,
<em>fp_max</em>] to the quantized range [<em>quantized_min</em>, <em>quantized_max</em>].
That is to say,</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_{\mathrm{max}} = S(q_{\mathrm{max}}-Z)\)</span></p>
<p><span class="math notranslate nohighlight">\(r_{\mathrm{min}} = S(q_{\mathrm{min}}-Z)\)</span></p>
</div></blockquote>
<p>Substracting these two equations, we have,</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(S=(r_{\mathrm{max}} - r_{\mathrm{min}}) / (q_{\mathrm{max}} - q_{\mathrm{min}})\)</span></p>
</div></blockquote>
<p>There are different approaches to determine the <span class="math notranslate nohighlight">\(r_{\mathrm{min}}\)</span>
and <span class="math notranslate nohighlight">\(r_{\mathrm{max}}\)</span> of a floating point tensor <code class="docutils literal notranslate"><span class="pre">fp_tensor</span></code>.</p>
<ul class="simple">
<li><p>The most common method is directly using the minimum and maximum
value of <code class="docutils literal notranslate"><span class="pre">fp_tensor</span></code>.</p></li>
<li><p>Another widely used method is minimizing Kullback-Leibler-J
divergence to determine the <em>fp_max</em>.</p></li>
</ul>
</section>
<section id="zero-point">
<h3>zero point<a class="headerlink" href="#zero-point" title="Link to this heading"></a></h3>
<p>Once we determine the scaling factor <span class="math notranslate nohighlight">\(S\)</span>, we can directly use the
relationship between <span class="math notranslate nohighlight">\(r_{\mathrm{min}}\)</span> and
<span class="math notranslate nohighlight">\(q_{\mathrm{min}}\)</span> to calculate the zero point <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Z = \mathrm{int}(\mathrm{round}(q_{\mathrm{min}} - r_{\mathrm{min}} / S))\)</span></p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_quantization_scale_and_zero_point</span><span class="p">(</span><span class="n">fp_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    get quantization scale for single tensor</span>
<span class="sd">    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized</span>
<span class="sd">    :param bitwidth: [int] quantization bit width</span>
<span class="sd">    :return:</span>
<span class="sd">        [float] scale</span>
<span class="sd">        [int] zero_point</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">fp_max</span> <span class="o">=</span> <span class="n">fp_tensor</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">fp_min</span> <span class="o">=</span> <span class="n">fp_tensor</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># hint: one line of code for calculating scale</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">fp_max</span> <span class="o">-</span> <span class="n">fp_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">quantized_max</span> <span class="o">-</span> <span class="n">quantized_min</span><span class="p">)</span>
    <span class="c1"># hint: one line of code for calculating zero_point</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quantized_min</span> <span class="o">-</span> <span class="n">fp_min</span> <span class="o">/</span> <span class="n">scale</span>


    <span class="c1"># clip the zero_point to fall in [quantized_min, quantized_max]</span>
    <span class="k">if</span> <span class="n">zero_point</span> <span class="o">&lt;</span> <span class="n">quantized_min</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quantized_min</span>
    <span class="k">elif</span> <span class="n">zero_point</span> <span class="o">&gt;</span> <span class="n">quantized_max</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quantized_max</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># convert from float to int using round()</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span>
</pre></div>
</div>
<p>We now wrap <code class="docutils literal notranslate"><span class="pre">linear_quantize()</span></code>and
<code class="docutils literal notranslate"><span class="pre">get_quantization_scale_and_zero_point()</span></code> into one function.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_quantize_feature</span><span class="p">(</span><span class="n">fp_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    linear quantization for feature tensor</span>
<span class="sd">    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized</span>
<span class="sd">    :param bitwidth: [int] quantization bit width</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.(cuda.)Tensor] quantized tensor</span>
<span class="sd">        [float] scale</span>
<span class="sd">        [int] zero_point</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">get_quantization_scale_and_zero_point</span><span class="p">(</span><span class="n">fp_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">linear_quantize</span><span class="p">(</span><span class="n">fp_tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span>
</pre></div>
</div>
</section>
</section>
<section id="special-case-linear-quantization-on-weight-tensor">
<h2>Special case: linear quantization on weight tensor<a class="headerlink" href="#special-case-linear-quantization-on-weight-tensor" title="Link to this heading"></a></h2>
<p>Let’s first see the distribution of weight values.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="c1"># bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256</span>
    <span class="k">if</span> <span class="n">bitwidth</span> <span class="o">&lt;=</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
        <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">align</span> <span class="o">=</span> <span class="s1">&#39;left&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bins</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="n">align</span> <span class="o">=</span> <span class="s1">&#39;mid&#39;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">plot_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">plot_index</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">align</span><span class="o">=</span><span class="n">align</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                    <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span> <span class="k">if</span> <span class="n">bitwidth</span> <span class="o">&lt;=</span> <span class="mi">4</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">bitwidth</span> <span class="o">&lt;=</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">quantized_min</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">quantized_min</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">quantized_max</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
            <span class="n">plot_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Histogram of Weights (bitwidth=</span><span class="si">{</span><span class="n">bitwidth</span><span class="si">}</span><span class="s1"> bits)&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.925</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">recover_model</span><span class="p">()</span>
<span class="n">plot_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/Quantization_59_0.png" src="../_images/Quantization_59_0.png" />
<p>As we can see from the histograms above, the distribution of weight
values are nearly symmetric about 0 (except for the classifier in this
case). Therefore, we usually make zero point <span class="math notranslate nohighlight">\(Z=0\)</span> when
quantizating the weights.</p>
<p>From <span class="math notranslate nohighlight">\(r = S(q-Z)\)</span>, we have</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_{\mathrm{max}} = S \cdot q_{\mathrm{max}}\)</span></p>
</div></blockquote>
<p>and then</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(S = r_{\mathrm{max}} / q_{\mathrm{max}}\)</span></p>
</div></blockquote>
<p>We directly use the maximum magnitude of weight values as
<span class="math notranslate nohighlight">\(r_{\mathrm{max}}\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_quantization_scale_for_weight</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    get quantization scale for single tensor of weight</span>
<span class="sd">    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized</span>
<span class="sd">    :param bitwidth: [integer] quantization bit width</span>
<span class="sd">    :return:</span>
<span class="sd">        [floating scalar] scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># we just assume values in weight are symmetric</span>
    <span class="c1"># we also always make zero_point 0 for weight</span>
    <span class="n">fp_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mf">5e-7</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">quantized_max</span> <span class="o">=</span> <span class="n">get_quantized_range</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fp_max</span> <span class="o">/</span> <span class="n">quantized_max</span>
</pre></div>
</div>
<section id="per-channel-linear-quantization">
<h3>Per-channel Linear Quantization<a class="headerlink" href="#per-channel-linear-quantization" title="Link to this heading"></a></h3>
<p>Recall that for 2D convolution, the weight tensor is a 4-D tensor in the
shape of (num_output_channels, num_input_channels, kernel_height,
kernel_width).</p>
<p>Intensive experiments show that using the different scaling factors
<span class="math notranslate nohighlight">\(S\)</span> and zero points <span class="math notranslate nohighlight">\(Z\)</span> for different output channels will
perform better. Therefore, we have to determine scaling factor <span class="math notranslate nohighlight">\(S\)</span>
and zero point <span class="math notranslate nohighlight">\(Z\)</span> for the subtensor of each output channel
independently.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_quantize_weight_per_channel</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    linear quantization for weight tensor</span>
<span class="sd">        using different scales and zero_points for different output channels</span>
<span class="sd">    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized</span>
<span class="sd">    :param bitwidth: [int] quantization bit width</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.(cuda.)Tensor] quantized tensor</span>
<span class="sd">        [torch.(cuda.)Tensor] scale tensor</span>
<span class="sd">        [int] zero point (which is always 0)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim_output_channels</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_output_channels</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim_output_channels</span><span class="p">]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_output_channels</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">oc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_output_channels</span><span class="p">):</span>
        <span class="n">_subtensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dim_output_channels</span><span class="p">,</span> <span class="n">oc</span><span class="p">)</span>
        <span class="n">_scale</span> <span class="o">=</span> <span class="n">get_quantization_scale_for_weight</span><span class="p">(</span><span class="n">_subtensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
        <span class="n">scale</span><span class="p">[</span><span class="n">oc</span><span class="p">]</span> <span class="o">=</span> <span class="n">_scale</span>
    <span class="n">scale_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="n">scale_shape</span><span class="p">[</span><span class="n">dim_output_channels</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">scale_shape</span><span class="p">)</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">linear_quantize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="mi">0</span>
</pre></div>
</div>
</section>
<section id="a-quick-peek-at-linear-quantization-on-weights">
<h3>A Quick Peek at Linear Quantization on Weights<a class="headerlink" href="#a-quick-peek-at-linear-quantization-on-weights" title="Link to this heading"></a></h3>
<p>Now let’s have a peek on the weight distribution and model size when
applying linear quantization on weights with different bitwidths.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">peek_linear_quantization</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">bitwidth</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">quantized_param</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> \
                    <span class="n">linear_quantize_weight_per_channel</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
                <span class="n">param</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">quantized_param</span><span class="p">)</span>
        <span class="n">plot_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">)</span>
        <span class="n">recover_model</span><span class="p">()</span>

<span class="n">peek_linear_quantization</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Quantization_65_0.png" src="../_images/Quantization_65_0.png" />
<img alt="../_images/Quantization_65_1.png" src="../_images/Quantization_65_1.png" />
</section>
</section>
<section id="quantized-inference">
<h2>Quantized Inference<a class="headerlink" href="#quantized-inference" title="Link to this heading"></a></h2>
<p>After quantization, the inference of convolution and fully-connected
layers also change.</p>
<p>Recall that <span class="math notranslate nohighlight">\(r = S(q-Z)\)</span>, and we have</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_{\mathrm{input}} = S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}})\)</span></p>
<p><span class="math notranslate nohighlight">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}(q_{\mathrm{weight}}-Z_{\mathrm{weight}})\)</span></p>
<p><span class="math notranslate nohighlight">\(r_{\mathrm{bias}} = S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</div></blockquote>
<p>Since <span class="math notranslate nohighlight">\(Z_{\mathrm{weight}}=0\)</span>,
<span class="math notranslate nohighlight">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}q_{\mathrm{weight}}\)</span>.</p>
<p>The floating point convolution can be written as,</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_{\mathrm{output}} = \mathrm{CONV}[r_{\mathrm{input}}, r_{\mathrm{weight}}] + r_{\mathrm{bias}}\\ \;\;\;\;\;\;\;\;= \mathrm{CONV}[S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}}), S_{\mathrm{weight}}q_{\mathrm{weight}}] + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\\ \;\;\;\;\;\;\;\;= \mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}]\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}}) + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</div></blockquote>
<p>To further simplify the computation, we could let</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math notranslate nohighlight">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</div></blockquote>
<p>so that</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}})\)</span>
<span class="math notranslate nohighlight">\(\;\;\;\;\;\;\;\;= (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}})\)</span></p>
</div></blockquote>
<p>Since &gt;
<span class="math notranslate nohighlight">\(r_{\mathrm{output}} = S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}})\)</span></p>
<p>we have &gt;
<span class="math notranslate nohighlight">\(S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}}) = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}})\)</span></p>
<p>and thus &gt;
<span class="math notranslate nohighlight">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
<p>Since <span class="math notranslate nohighlight">\(Z_{\mathrm{input}}\)</span>, <span class="math notranslate nohighlight">\(q_{\mathrm{weight}}\)</span>,
<span class="math notranslate nohighlight">\(q_{\mathrm{bias}}\)</span> are determined before inference, let</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span></p>
</div></blockquote>
<p>we have</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}}) \cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</div></blockquote>
<p>Similarily, for fully-connected layer, we have</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</div></blockquote>
<p>where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{Linear}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span></p>
</div></blockquote>
<p>From the above deduction, we know that</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math notranslate nohighlight">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_quantize_bias_per_output_channel</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">input_scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    linear quantization for single bias tensor</span>
<span class="sd">        quantized_bias = fp_bias / bias_scale</span>
<span class="sd">    :param bias: [torch.FloatTensor] bias weight to be quantized</span>
<span class="sd">    :param weight_scale: [float or torch.FloatTensor] weight scale tensor</span>
<span class="sd">    :param input_scale: [float] input scale</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.IntTensor] quantized bias tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_scale</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">weight_scale</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
        <span class="n">weight_scale</span> <span class="o">=</span> <span class="n">weight_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">weight_scale</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>


    <span class="n">bias_scale</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">input_scale</span>


    <span class="n">quantized_bias</span> <span class="o">=</span> <span class="n">linear_quantize</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">bias_scale</span><span class="p">,</span>
                                     <span class="n">zero_point</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_bias</span><span class="p">,</span> <span class="n">bias_scale</span><span class="p">,</span> <span class="mi">0</span>
</pre></div>
</div>
<section id="quantized-fully-connected-layer">
<h3>Quantized Fully-Connected Layer<a class="headerlink" href="#quantized-fully-connected-layer" title="Link to this heading"></a></h3>
<p>For quantized fully-connected layer, we first precompute
<span class="math notranslate nohighlight">\(Q_{\mathrm{bias}}\)</span>. Recall that
<span class="math notranslate nohighlight">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{Linear}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shift_quantized_linear_bias</span><span class="p">(</span><span class="n">quantized_bias</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_zero_point</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    shift quantized bias to incorporate input_zero_point for nn.Linear</span>
<span class="sd">        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)</span>
<span class="sd">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span>
<span class="sd">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span>
<span class="sd">    :param input_zero_point: [int] input zero point</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.IntTensor] shifted quantized bias tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">quantized_bias</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">quantized_bias</span> <span class="o">-</span> <span class="n">quantized_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_zero_point</span>
</pre></div>
</div>
<p><strong>Hint</strong>:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantized_linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">feature_bitwidth</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="p">,</span>
                     <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
                     <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    quantized fully-connected layer</span>
<span class="sd">    :param input: [torch.CharTensor] quantized input (torch.int8)</span>
<span class="sd">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span>
<span class="sd">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span>
<span class="sd">    :param feature_bitwidth: [int] quantization bit width of input and output</span>
<span class="sd">    :param weight_bitwidth: [int] quantization bit width of weight</span>
<span class="sd">    :param input_zero_point: [int] input zero point</span>
<span class="sd">    :param output_zero_point: [int] output zero point</span>
<span class="sd">    :param input_scale: [float] input feature scale</span>
<span class="sd">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span>
<span class="sd">    :param output_scale: [float] output feature scale</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.CharIntTensor] quantized output feature (torch.int8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">bias</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">output_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">output_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">weight_scale</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="c1"># Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)</span>
    <span class="k">if</span> <span class="s1">&#39;cpu&#39;</span> <span class="ow">in</span> <span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">:</span>
        <span class="c1"># use 32-b MAC for simplicity</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># current version pytorch does not yet support integer-based linear() on GPUs</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">bias</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>


    <span class="c1"># Step 2: scale the output</span>
    <span class="c1">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span>
    <span class="c1">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_scale</span> <span class="o">*</span> <span class="n">weight_scale</span> <span class="o">/</span> <span class="n">output_scale</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Step 3: shift output by output_zero_point</span>
    <span class="c1">#         hint: one line of code</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">output_zero_point</span>


    <span class="c1"># Make sure all value lies in the bitwidth-bit range</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">*</span><span class="n">get_quantized_range</span><span class="p">(</span><span class="n">feature_bitwidth</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>Let’s verify the functionality of defined quantized fully connected
layer.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_quantized_fc</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*</span> <span class="n">Test</span> <span class="n">quantized_fc</span><span class="p">()</span>
    <span class="n">target</span> <span class="n">bitwidth</span><span class="p">:</span> <span class="mi">2</span> <span class="n">bits</span>
      <span class="n">batch</span> <span class="n">size</span><span class="p">:</span> <span class="mi">4</span>
      <span class="nb">input</span> <span class="n">channels</span><span class="p">:</span> <span class="mi">8</span>
      <span class="n">output</span> <span class="n">channels</span><span class="p">:</span> <span class="mi">8</span>
<span class="o">*</span> <span class="n">Test</span> <span class="n">passed</span><span class="o">.</span>
</pre></div>
</div>
<img alt="../_images/Quantization_76_1.png" src="../_images/Quantization_76_1.png" />
</section>
<section id="quantized-convolution">
<h3>Quantized Convolution<a class="headerlink" href="#quantized-convolution" title="Link to this heading"></a></h3>
<p>For quantized convolution layer, we first precompute
<span class="math notranslate nohighlight">\(Q_{\mathrm{bias}}\)</span>. Recall that
<span class="math notranslate nohighlight">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shift_quantized_conv2d_bias</span><span class="p">(</span><span class="n">quantized_bias</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_zero_point</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    shift quantized bias to incorporate input_zero_point for nn.Conv2d</span>
<span class="sd">        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)</span>
<span class="sd">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span>
<span class="sd">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span>
<span class="sd">    :param input_zero_point: [int] input zero point</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.IntTensor] shifted quantized bias tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">quantized_bias</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">quantized_bias</span> <span class="o">-</span> <span class="n">quantized_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_zero_point</span>
</pre></div>
</div>
<p><strong>Hint</strong>: &gt;
<span class="math notranslate nohighlight">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}}) \cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantized_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">feature_bitwidth</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="p">,</span>
                     <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
                     <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span>
                     <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    quantized 2d convolution</span>
<span class="sd">    :param input: [torch.CharTensor] quantized input (torch.int8)</span>
<span class="sd">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span>
<span class="sd">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span>
<span class="sd">    :param feature_bitwidth: [int] quantization bit width of input and output</span>
<span class="sd">    :param weight_bitwidth: [int] quantization bit width of weight</span>
<span class="sd">    :param input_zero_point: [int] input zero point</span>
<span class="sd">    :param output_zero_point: [int] output zero point</span>
<span class="sd">    :param input_scale: [float] input feature scale</span>
<span class="sd">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span>
<span class="sd">    :param output_scale: [float] output feature scale</span>
<span class="sd">    :return:</span>
<span class="sd">        [torch.(cuda.)CharTensor] quantized output feature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">bias</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">output_zero_point</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">output_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">weight_scale</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="c1"># Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">input_zero_point</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;cpu&#39;</span> <span class="ow">in</span> <span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">:</span>
        <span class="c1"># use 32-b MAC for simplicity</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># current version pytorch does not yet support integer-based conv2d() on GPUs</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


    <span class="c1"># hint: this code block should be the very similar to quantized_linear()</span>

    <span class="c1"># Step 2: scale the output</span>
    <span class="c1">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span>
    <span class="c1">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_scale</span> <span class="o">*</span> <span class="n">weight_scale</span> <span class="o">/</span> <span class="n">output_scale</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Step 3: shift output by output_zero_point</span>
    <span class="c1">#         hint: one line of code</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">output_zero_point</span>


    <span class="c1"># Make sure all value lies in the bitwidth-bit range</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">*</span><span class="n">get_quantized_range</span><span class="p">(</span><span class="n">feature_bitwidth</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>Finally, we are putting everything together and perform post-training
<code class="docutils literal notranslate"><span class="pre">int8</span></code> quantization for the model. We will convert the convolutional
and linear layers in the model to a quantized version one-by-one.</p>
<ol class="arabic simple">
<li><p>Firstly, we will fuse a BatchNorm layer into its previous
convolutional layer, which is a standard practice before
quantization. Fusing batchnorm reduces the extra multiplication
during inference.</p></li>
</ol>
<p>We will also verify that the fused model <code class="docutils literal notranslate"><span class="pre">model_fused</span></code> has the same
accuracy as the original model (BN fusion is an equivalent transform
that does not change network functionality).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fuse_conv_bn</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">bn</span><span class="p">):</span>
    <span class="c1"># modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html</span>
    <span class="k">assert</span> <span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="n">factor</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">running_var</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">bn</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">factor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="o">-</span> <span class="n">bn</span><span class="o">.</span><span class="n">running_mean</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">factor</span> <span class="o">+</span> <span class="n">bn</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">conv</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before conv-bn fusion: backbone length&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="p">))</span>
<span class="c1">#  fuse the batchnorm into conv layers</span>
<span class="n">recover_model</span><span class="p">()</span>
<span class="n">model_fused</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">fused_backbone</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ptr</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">ptr</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">fused_backbone</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fuse_conv_bn</span><span class="p">(</span>
            <span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">],</span> <span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
        <span class="n">ptr</span> <span class="o">+=</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fused_backbone</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">])</span>
        <span class="n">ptr</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">fused_backbone</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After conv-bn fusion: backbone length&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_fused</span><span class="o">.</span><span class="n">backbone</span><span class="p">))</span>
<span class="c1"># sanity check, no BN anymore</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model_fused</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">)</span>

<span class="c1">#  the accuracy will remain the same after fusion</span>
<span class="n">fused_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model_fused</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy of the fused model=</span><span class="si">{</span><span class="n">fused_acc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Before</span> <span class="n">conv</span><span class="o">-</span><span class="n">bn</span> <span class="n">fusion</span><span class="p">:</span> <span class="n">backbone</span> <span class="n">length</span> <span class="mi">29</span>
<span class="n">After</span> <span class="n">conv</span><span class="o">-</span><span class="n">bn</span> <span class="n">fusion</span><span class="p">:</span> <span class="n">backbone</span> <span class="n">length</span> <span class="mi">21</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">fused</span> <span class="n">model</span><span class="o">=</span><span class="mf">92.95</span><span class="o">%</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>We will run the model with some sample data to get the range of each
feature map, so that we can get the range of the feature maps and
compute their corresponding scaling factors and zero points.</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add hook to record the min max value of the activation</span>
<span class="n">input_activation</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">add_range_recoder_hook</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">functools</span>
    <span class="k">def</span> <span class="nf">_record_range</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">module_name</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">input_activation</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">output_activation</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="n">all_hooks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)):</span>
            <span class="n">all_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_record_range</span><span class="p">,</span> <span class="n">module_name</span><span class="o">=</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">all_hooks</span>

<span class="n">hooks</span> <span class="o">=</span> <span class="n">add_range_recoder_hook</span><span class="p">(</span><span class="n">model_fused</span><span class="p">)</span>
<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span><span class="o">.</span><span class="fm">__next__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model_fused</span><span class="p">(</span><span class="n">sample_data</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>

<span class="c1"># remove hooks</span>
<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hooks</span><span class="p">:</span>
    <span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Finally, let’s do model quantization. We will convert the model in
the following mapping</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span> <span class="n">QuantizedConv2d</span><span class="p">,</span>
<span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span> <span class="n">QuantizedLinear</span><span class="p">,</span>
<span class="c1"># the following twos are just wrappers, as current</span>
<span class="c1"># torch modules do not support int8 data format;</span>
<span class="c1"># we will temporarily convert them to fp32 for computation</span>
<span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">:</span> <span class="n">QuantizedMaxPool2d</span><span class="p">,</span>
<span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">:</span> <span class="n">QuantizedAvgPool2d</span><span class="p">,</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QuantizedConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span>
                 <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
                 <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span>
                 <span class="n">feature_bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># current version Pytorch does not support IntTensor as nn.Parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_zero_point</span> <span class="o">=</span> <span class="n">input_zero_point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_zero_point</span> <span class="o">=</span> <span class="n">output_zero_point</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_scale</span> <span class="o">=</span> <span class="n">input_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight_scale&#39;</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_scale</span> <span class="o">=</span> <span class="n">output_scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_bitwidth</span> <span class="o">=</span> <span class="n">feature_bitwidth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_bitwidth</span> <span class="o">=</span> <span class="n">weight_bitwidth</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">quantized_conv2d</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_bitwidth</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_bitwidth</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_zero_point</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_scale</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">)</span>

<span class="k">class</span> <span class="nc">QuantizedLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span>
                 <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
                 <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span>
                 <span class="n">feature_bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># current version Pytorch does not support IntTensor as nn.Parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_zero_point</span> <span class="o">=</span> <span class="n">input_zero_point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_zero_point</span> <span class="o">=</span> <span class="n">output_zero_point</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_scale</span> <span class="o">=</span> <span class="n">input_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight_scale&#39;</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_scale</span> <span class="o">=</span> <span class="n">output_scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_bitwidth</span> <span class="o">=</span> <span class="n">feature_bitwidth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_bitwidth</span> <span class="o">=</span> <span class="n">weight_bitwidth</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">quantized_linear</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_bitwidth</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_bitwidth</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_zero_point</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_zero_point</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_scale</span>
            <span class="p">)</span>

<span class="k">class</span> <span class="nc">QuantizedMaxPool2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># current version PyTorch does not support integer-based MaxPool</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">QuantizedAvgPool2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># current version PyTorch does not support integer-based AvgPool</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

<span class="c1"># we use int8 quantization, which is quite popular</span>
<span class="n">feature_bitwidth</span> <span class="o">=</span> <span class="n">weight_bitwidth</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_fused</span><span class="p">)</span>
<span class="n">quantized_backbone</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ptr</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">ptr</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">):</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]</span>
        <span class="n">conv_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;backbone.</span><span class="si">{</span><span class="n">ptr</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">relu_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;backbone.</span><span class="si">{</span><span class="n">ptr</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="n">input_scale</span><span class="p">,</span> <span class="n">input_zero_point</span> <span class="o">=</span> \
            <span class="n">get_quantization_scale_and_zero_point</span><span class="p">(</span>
                <span class="n">input_activation</span><span class="p">[</span><span class="n">conv_name</span><span class="p">],</span> <span class="n">feature_bitwidth</span><span class="p">)</span>

        <span class="n">output_scale</span><span class="p">,</span> <span class="n">output_zero_point</span> <span class="o">=</span> \
            <span class="n">get_quantization_scale_and_zero_point</span><span class="p">(</span>
                <span class="n">output_activation</span><span class="p">[</span><span class="n">relu_name</span><span class="p">],</span> <span class="n">feature_bitwidth</span><span class="p">)</span>

        <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">weight_zero_point</span> <span class="o">=</span> \
            <span class="n">linear_quantize_weight_per_channel</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="p">)</span>
        <span class="n">quantized_bias</span><span class="p">,</span> <span class="n">bias_scale</span><span class="p">,</span> <span class="n">bias_zero_point</span> <span class="o">=</span> \
            <span class="n">linear_quantize_bias_per_output_channel</span><span class="p">(</span>
                <span class="n">conv</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">input_scale</span><span class="p">)</span>
        <span class="n">shifted_quantized_bias</span> <span class="o">=</span> \
            <span class="n">shift_quantized_conv2d_bias</span><span class="p">(</span><span class="n">quantized_bias</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span>
                                        <span class="n">input_zero_point</span><span class="p">)</span>

        <span class="n">quantized_conv</span> <span class="o">=</span> <span class="n">QuantizedConv2d</span><span class="p">(</span>
            <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">shifted_quantized_bias</span><span class="p">,</span>
            <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
            <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span>
            <span class="n">conv</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">feature_bitwidth</span><span class="o">=</span><span class="n">feature_bitwidth</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="o">=</span><span class="n">weight_bitwidth</span>
        <span class="p">)</span>

        <span class="n">quantized_backbone</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">quantized_conv</span><span class="p">)</span>
        <span class="n">ptr</span> <span class="o">+=</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">):</span>
        <span class="n">quantized_backbone</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">QuantizedMaxPool2d</span><span class="p">(</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]</span><span class="o">.</span><span class="n">stride</span>
            <span class="p">))</span>
        <span class="n">ptr</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">):</span>
        <span class="n">quantized_backbone</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">QuantizedAvgPool2d</span><span class="p">(</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]</span><span class="o">.</span><span class="n">stride</span>
            <span class="p">))</span>
        <span class="n">ptr</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span><span class="p">[</span><span class="n">ptr</span><span class="p">]))</span>  <span class="c1"># should not happen</span>
<span class="n">quantized_model</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">quantized_backbone</span><span class="p">)</span>

<span class="c1"># finally, quantized the classifier</span>
<span class="n">fc_name</span> <span class="o">=</span> <span class="s1">&#39;classifier&#39;</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span>
<span class="n">input_scale</span><span class="p">,</span> <span class="n">input_zero_point</span> <span class="o">=</span> \
    <span class="n">get_quantization_scale_and_zero_point</span><span class="p">(</span>
        <span class="n">input_activation</span><span class="p">[</span><span class="n">fc_name</span><span class="p">],</span> <span class="n">feature_bitwidth</span><span class="p">)</span>

<span class="n">output_scale</span><span class="p">,</span> <span class="n">output_zero_point</span> <span class="o">=</span> \
    <span class="n">get_quantization_scale_and_zero_point</span><span class="p">(</span>
        <span class="n">output_activation</span><span class="p">[</span><span class="n">fc_name</span><span class="p">],</span> <span class="n">feature_bitwidth</span><span class="p">)</span>

<span class="n">quantized_weight</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">weight_zero_point</span> <span class="o">=</span> \
    <span class="n">linear_quantize_weight_per_channel</span><span class="p">(</span><span class="n">fc</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="p">)</span>
<span class="n">quantized_bias</span><span class="p">,</span> <span class="n">bias_scale</span><span class="p">,</span> <span class="n">bias_zero_point</span> <span class="o">=</span> \
    <span class="n">linear_quantize_bias_per_output_channel</span><span class="p">(</span>
        <span class="n">fc</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">input_scale</span><span class="p">)</span>
<span class="n">shifted_quantized_bias</span> <span class="o">=</span> \
    <span class="n">shift_quantized_linear_bias</span><span class="p">(</span><span class="n">quantized_bias</span><span class="p">,</span> <span class="n">quantized_weight</span><span class="p">,</span>
                                <span class="n">input_zero_point</span><span class="p">)</span>

<span class="n">quantized_model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span>
    <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">shifted_quantized_bias</span><span class="p">,</span>
    <span class="n">input_zero_point</span><span class="p">,</span> <span class="n">output_zero_point</span><span class="p">,</span>
    <span class="n">input_scale</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="n">output_scale</span><span class="p">,</span>
    <span class="n">feature_bitwidth</span><span class="o">=</span><span class="n">feature_bitwidth</span><span class="p">,</span> <span class="n">weight_bitwidth</span><span class="o">=</span><span class="n">weight_bitwidth</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The quantization process is done! Let’s print and visualize the model
architecture and also verify the accuracy of the quantized model.</p>
<p>To run the quantized model, we need an extra preprocessing to map the
input data from range (0, 1) into <code class="docutils literal notranslate"><span class="pre">int8</span></code> range of (-128, 127). Fill in
the code below to finish the extra preprocessing.</p>
<p><strong>Hint</strong>: you should find that the quantized model has roughly the same
accuracy as the <code class="docutils literal notranslate"><span class="pre">fp32</span></code> counterpart.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">extra_preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># hint: you need to convert the original fp32 input of range (0, 1)</span>
    <span class="c1">#  into int8 format of range (-128, 127)</span>
    <span class="c1">############### YOUR CODE STARTS HERE ###############</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">255</span> <span class="o">-</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="c1">############### YOUR CODE ENDS HERE #################</span>

<span class="n">int8_model_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span>
                               <span class="n">extra_preprocess</span><span class="o">=</span><span class="p">[</span><span class="n">extra_preprocess</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;int8 model has accuracy=</span><span class="si">{</span><span class="n">int8_model_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">VGG</span><span class="p">(</span>
  <span class="p">(</span><span class="n">backbone</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">QuantizedMaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">QuantizedMaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="n">QuantizedMaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="n">QuantizedConv2d</span><span class="p">()</span>
    <span class="p">(</span><span class="mi">11</span><span class="p">):</span> <span class="n">QuantizedMaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">12</span><span class="p">):</span> <span class="n">QuantizedAvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">QuantizedLinear</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>eval:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">int8</span> <span class="n">model</span> <span class="n">has</span> <span class="n">accuracy</span><span class="o">=</span><span class="mf">92.90</span><span class="o">%</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Pruning.html" class="btn btn-neutral float-left" title="Pruning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Pipeline-Tutorial.html" class="btn btn-neutral float-right" title="sconce Pipeline Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Sathyaprakash.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>