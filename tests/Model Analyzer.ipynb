{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0da01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df72568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def name_fixer(names):\n",
    "    \"\"\"\n",
    "    Fix the names by removing the indices in square brackets.\n",
    "    Args:\n",
    "      names (list): List of names.\n",
    "\n",
    "    Returns:\n",
    "      list: List of fixed names.\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    for string in names:\n",
    "        matches = re.finditer(r'\\.\\[(\\d+)\\]', string)\n",
    "        pop_list = [m.start(0) for m in matches]\n",
    "        pop_list.sort(reverse=True)\n",
    "        if len(pop_list) > 0:\n",
    "            string = list(string)\n",
    "            for pop_id in pop_list:\n",
    "                string.pop(pop_id)\n",
    "            string = ''.join(string)\n",
    "        return_list.append(string)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "\n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                # conv-bn-relu\n",
    "                add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add(\"bn\", nn.BatchNorm2d(x))\n",
    "                add(\"relu\", nn.ReLU(True))\n",
    "                in_channels = x\n",
    "            else:\n",
    "                # maxpool\n",
    "                add(\"pool\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2, 3])\n",
    "\n",
    "        # classifier: [N, 512] => [N, 10]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "fusing_layers = [\n",
    "    'Conv2d',\n",
    "    'BatchNorm2d',\n",
    "    'ReLU',\n",
    "    'Linear',\n",
    "    'BatchNorm1d',\n",
    "]\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_all_layers(model, parent_name=''):\n",
    "    layers = []\n",
    "    for name, module in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        test_name = \"model.\" + full_name\n",
    "        try:\n",
    "            eval(test_name)\n",
    "            layers.append((full_name, module))\n",
    "        except:\n",
    "            layers.append((reformat_layer_name(full_name), module))\n",
    "        if isinstance(module, nn.Module):\n",
    "            layers.extend(get_all_layers(module, parent_name=full_name))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def reformat_layer_name(str_data):\n",
    "    try:\n",
    "        split_data = str_data.split('.')\n",
    "        for ind in range(len(split_data)):\n",
    "            data = split_data[ind]\n",
    "            if (data.isdigit()):\n",
    "                split_data[ind] = \"[\" + data + \"]\"\n",
    "        final_string = '.'.join(split_data)\n",
    "\n",
    "        iters_a = re.finditer(r'[a-zA-Z]\\.\\[', final_string)\n",
    "        indices = [m.start(0) + 1 for m in iters_a]\n",
    "        iters = re.finditer(r'\\]\\.\\[', final_string)\n",
    "        indices.extend([m.start(0) + 1 for m in iters])\n",
    "\n",
    "        final_string = list(final_string)\n",
    "        final_string = [final_string[i] for i in range(len(final_string)) if i not in indices]\n",
    "\n",
    "        str_data = ''.join(final_string)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return str_data\n",
    "\n",
    "\n",
    "def summary_string_fixed(model, all_layers, input_size, model_name=None, batch_size=-1, dtypes=None):\n",
    "    if dtypes is None:\n",
    "        dtypes = [torch.FloatTensor] * len(input_size)\n",
    "\n",
    "    def register_hook(module, module_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal module_idx\n",
    "            m_key = all_layers[module_idx][0]\n",
    "            m_key = model_name + \".\" + m_key\n",
    "\n",
    "            try:\n",
    "                eval(m_key)\n",
    "            except:\n",
    "                m_key = name_fixer([m_key])[0]\n",
    "\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"type\"] = str(type(module)).split('.')[-1][:-2]\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "                summary[m_key][\"weight_shape\"] = module.weight.shape\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    for module_idx, (layer_name, module) in enumerate(all_layers):\n",
    "        register_hook(module, module_idx)\n",
    "\n",
    "    model(*x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_input_channel_importance(weight):\n",
    "    importances = []\n",
    "    for i_c in range(weight.shape[1]):\n",
    "        channel_weight = weight.detach()[:, i_c]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "\n",
    "def get_importance(layer, sparsity):\n",
    "    sorted_indices = torch.argsort(get_input_channel_importance(layer.weight), descending=True)\n",
    "    n_keep = int(round(len(sorted_indices) * (1.0 - sparsity)))\n",
    "    indices_to_keep = sorted_indices[:n_keep]\n",
    "    return indices_to_keep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_cwp(model, pruning_ratio_list):\n",
    "    \n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "\n",
    "    def get_layer_name(obj):\n",
    "        if isinstance(obj, list):\n",
    "            layer_list = []\n",
    "            for internal_layer in obj:\n",
    "                layer_list.append(eval(internal_layer.replace('model', 'pruned_model')))\n",
    "            return layer_list\n",
    "        else:\n",
    "            nonlocal pruned_model\n",
    "            return eval(obj.replace('model', 'pruned_model'))\n",
    "\n",
    "    for list_ind in range(len(possible_indices_ranges)):\n",
    "        sparsity = pruning_ratio_list[list_ind]\n",
    "        layer_list = np.asarray(possible_indices_ranges[list_ind])\n",
    "\n",
    "        prev_conv = get_layer_name(layer_list[0, 0])\n",
    "        prev_bn = get_layer_name(layer_list[0, 1])\n",
    "        next_convs = [c for c in get_layer_name(list(layer_list[1:, 0]))]\n",
    "        next_bns = [b for b in get_layer_name(list(layer_list[1:-1, 1]))]  # Avoid last 0\n",
    "\n",
    "        if (len(next_bns) == 0):\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, []])\n",
    "        else:\n",
    "            iter_layers = zip([prev_conv, prev_bn], [next_convs, next_bns])\n",
    "\n",
    "        importance_list_indices = get_importance(layer=next_convs[-1], sparsity=sparsity)\n",
    "\n",
    "        def prune_bn(layer, importance_list_indices):\n",
    "\n",
    "            layer.weight.set_(layer.weight.detach()[importance_list_indices])\n",
    "            layer.bias.set_(layer.bias.detach()[importance_list_indices])\n",
    "            layer.running_mean.set_(layer.running_mean.detach()[importance_list_indices])\n",
    "            layer.running_var.set_(layer.running_var.detach()[importance_list_indices])\n",
    "\n",
    "        for prev_layer, next_layers in iter_layers:\n",
    "            if(prev_layer != 0): #No BatchNorm Used:\n",
    "                \n",
    "                if (str(type(prev_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                    prune_bn(prev_layer, importance_list_indices)\n",
    "                else:\n",
    "                    prev_layer.weight.set_(prev_conv.weight.detach()[importance_list_indices, :])\n",
    "                    if prev_layer.bias is not None:\n",
    "                                bias_shape = prev_layer.weight.shape[0]\n",
    "                                prev_layer.bias = nn.Parameter(prev_layer.bias[:bias_shape])\n",
    "                    \n",
    "\n",
    "            if (len(next_layers) != 0):\n",
    "                for next_layer in next_layers:\n",
    "                    if (str(type(next_layer)).split('.')[-1][:-2] == 'BatchNorm2d'):  # BatchNorm2d\n",
    "                        prune_bn(next_layer, importance_list_indices)\n",
    "                    else:\n",
    "                        if (next_layer.weight.shape[1] == 1):\n",
    "                            \n",
    "\n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[importance_list_indices, :])\n",
    "                            number_of_channels = len(importance_list_indices)\n",
    "                            next_layer.groups = number_of_channels\n",
    "                           \n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "     \n",
    "                              \n",
    "                        else:\n",
    "                            \n",
    "                            next_layer.weight.set_(next_layer.weight.detach()[:, importance_list_indices])\n",
    "#                             next_layer.groups = len(importance_list_indices)\n",
    "                            if next_layer.bias is not None:\n",
    "                                bias_shape = next_layer.weight.shape[0]\n",
    "                                next_layer.bias = nn.Parameter(next_layer.bias[:bias_shape])\n",
    "                \n",
    "#                             if(next_layer.bias!=None):\n",
    "#                                 next_layer.bias.set_(next_layer.bias[:next_layer.weight.shape[1]])\n",
    "#                                 print(len(next_layer.bias))\n",
    "\n",
    "    return pruned_model, model\n",
    "\n",
    "\n",
    "def layer_mapping(model):\n",
    "    all_layers = get_all_layers(model)\n",
    "    model_summary = summary_string_fixed(model, all_layers, (3, 64, 64), model_name='model')  # , device=\"cuda\")\n",
    "\n",
    "    name_type_shape = []\n",
    "    for key in model_summary.keys():\n",
    "        data = model_summary[key]\n",
    "        if (\"weight_shape\" in data.keys()):\n",
    "            name_type_shape.append([key, data['type'], data['weight_shape'][0]])\n",
    "        #     else:\n",
    "    #         name_type_shape.append([key, data['type'], 0 ])\n",
    "    name_type_shape = np.asarray(name_type_shape)\n",
    "\n",
    "    name_list = name_type_shape[:, 0]\n",
    "\n",
    "    r_name_list = np.asarray(name_list)\n",
    "    random_picks = np.random.randint(0, len(r_name_list), 10)\n",
    "    test_name_list = r_name_list[random_picks]\n",
    "    eval_hit = False\n",
    "    for layer in test_name_list:\n",
    "        try:\n",
    "            eval(layer)\n",
    "\n",
    "        except:\n",
    "            eval_hit = True\n",
    "            break\n",
    "    if (eval_hit):\n",
    "        fixed_name_list = name_fixer(r_name_list)\n",
    "        name_type_shape[:, 0] = fixed_name_list\n",
    "\n",
    "    layer_types = name_type_shape[:, 1]\n",
    "    layer_shapes = name_type_shape[:, 2]\n",
    "    mapped_layers = {'model_layer': [], 'Conv2d_BatchNorm2d_ReLU': [], 'Conv2d_BatchNorm2d': [], 'Linear_ReLU': [],\n",
    "                     'Linear_BatchNorm1d': []}\n",
    "\n",
    "    def detect_sequences(lst):\n",
    "        i = 0\n",
    "        while i < len(lst):\n",
    "\n",
    "            if i + 2 < len(lst) and [l for l in lst[i: i + 3]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d_ReLU'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 3)]).tolist()\n",
    "                    )\n",
    "                    i += 3\n",
    "\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[0],\n",
    "                fusing_layers[1],\n",
    "            ]:\n",
    "                test_layer = layer_shapes[i: i + 2]\n",
    "                if (np.all(test_layer == test_layer[0])):\n",
    "                    mapped_layers['Conv2d_BatchNorm2d'].append(\n",
    "                        np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                    )\n",
    "                    i += 2\n",
    "            # if i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[0], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            # elif i + 1 < len(lst) and [ type(l) for l in lst[i:i+2]] == [fusing_layers[1], fusing_layers[2]]:\n",
    "            #     detected_sequences.append(np.take(name_list,[i for i in range(i,i+2)]).tolist())\n",
    "            #     i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[2],\n",
    "            ]:\n",
    "                mapped_layers['Linear_ReLU'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            elif i + 1 < len(lst) and [l for l in lst[i: i + 2]] == [\n",
    "                fusing_layers[3],\n",
    "                fusing_layers[4],\n",
    "            ]:\n",
    "                mapped_layers['Linear_BatchNorm1d'].append(\n",
    "                    np.take(name_list, [i for i in range(i, i + 2)]).tolist()\n",
    "                )\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    detect_sequences(layer_types)\n",
    "\n",
    "    for keys, value in mapped_layers.items():\n",
    "        mapped_layers[keys] = np.asarray(mapped_layers[keys])\n",
    "\n",
    "    mapped_layers['name_type_shape'] = name_type_shape\n",
    "    # self.mapped_layers = mapped_layers\n",
    "\n",
    "    # CWP\n",
    "    keys_to_lookout = ['Conv2d_BatchNorm2d_ReLU', 'Conv2d_BatchNorm2d']\n",
    "    pruning_layer_of_interest, qat_layer_of_interest = [], []\n",
    "\n",
    "    # CWP or QAT Fusion Layers\n",
    "    for keys in keys_to_lookout:\n",
    "        data = mapped_layers[keys]\n",
    "        if (len(data) != 0):\n",
    "            qat_layer_of_interest.append(data)\n",
    "    mapped_layers['qat_layers'] = np.asarray(qat_layer_of_interest)\n",
    "\n",
    "    return mapped_layers\n",
    "\n",
    "\n",
    "# GMP\n",
    "#         layer_of_interest=mapped_layers['name_type_shape'][:,0] # all layers with weights\n",
    "#         Check for all with weights\n",
    "# Wanda\n",
    "\n",
    "def string_fixer(name_list):\n",
    "    for ind in range(len(name_list)):\n",
    "        modified_string = re.sub(r'\\.(\\[)', r'\\1', name_list[ind])\n",
    "        name_list[ind] = modified_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ab0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def cwp_possible_layers(layer_name_list):\n",
    "    possible_indices = []\n",
    "    idx = 0\n",
    "    \n",
    "    while idx < len(layer_name_list):\n",
    "        current_value = layer_name_list[idx]\n",
    "        layer_shape = eval(current_value).weight.shape\n",
    "        curr_merge_list = []\n",
    "        curr_merge_list.append([current_value, 0])\n",
    "        hit_catch = False\n",
    "        for internal_idx in range(idx + 1, len(layer_name_list) - 1):\n",
    "            new_layer = layer_name_list[internal_idx]\n",
    "            new_layer_shape = eval(new_layer).weight.shape\n",
    "            if len(new_layer_shape) == 4:\n",
    "                curr_merge_list.append([new_layer, 0])\n",
    "                if layer_shape[0] == new_layer_shape[1]:\n",
    "                    hit_catch = True\n",
    "                    break\n",
    "            elif len(new_layer_shape) == 1:\n",
    "                curr_merge_list[len(curr_merge_list) - 1][1] = new_layer\n",
    "        possible_indices.append(curr_merge_list)\n",
    "        if hit_catch == True:\n",
    "            idx = internal_idx\n",
    "        else:\n",
    "            idx += 1\n",
    "    return possible_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7585e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sathya/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #load the pretrained model\n",
    "\n",
    "# resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# densenet = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "\n",
    "super_net_name = \"ofa_supernet_mbv3_w10\" \n",
    "\n",
    "vgg = VGG()\n",
    "mobilenet_v2 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "mobilenet_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "# super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True)\n",
    "model_list = [vgg, mobilenet_v2, mobilenet_v3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21540da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]          73,728\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "              ReLU-6          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 256, 16, 16]         294,912\n",
      "       BatchNorm2d-9          [-1, 256, 16, 16]             512\n",
      "             ReLU-10          [-1, 256, 16, 16]               0\n",
      "           Conv2d-11          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-12          [-1, 256, 16, 16]             512\n",
      "             ReLU-13          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-16            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-17            [-1, 512, 8, 8]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-20            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-24            [-1, 512, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-28            [-1, 512, 2, 2]               0\n",
      "           Linear-29                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 9,228,362\n",
      "Trainable params: 9,228,362\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.83\n",
      "Params size (MB): 35.20\n",
      "Estimated Total Size (MB): 45.04\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 32, 32]             162\n",
      "       BatchNorm2d-2            [-1, 6, 32, 32]              12\n",
      "              ReLU-3            [-1, 6, 32, 32]               0\n",
      "            Conv2d-4           [-1, 13, 32, 32]             702\n",
      "       BatchNorm2d-5           [-1, 13, 32, 32]              26\n",
      "              ReLU-6           [-1, 13, 32, 32]               0\n",
      "         MaxPool2d-7           [-1, 13, 16, 16]               0\n",
      "            Conv2d-8           [-1, 26, 16, 16]           3,042\n",
      "       BatchNorm2d-9           [-1, 26, 16, 16]              52\n",
      "             ReLU-10           [-1, 26, 16, 16]               0\n",
      "           Conv2d-11           [-1, 26, 16, 16]           6,084\n",
      "      BatchNorm2d-12           [-1, 26, 16, 16]              52\n",
      "             ReLU-13           [-1, 26, 16, 16]               0\n",
      "        MaxPool2d-14             [-1, 26, 8, 8]               0\n",
      "           Conv2d-15             [-1, 51, 8, 8]          11,934\n",
      "      BatchNorm2d-16             [-1, 51, 8, 8]             102\n",
      "             ReLU-17             [-1, 51, 8, 8]               0\n",
      "           Conv2d-18             [-1, 51, 8, 8]          23,409\n",
      "      BatchNorm2d-19             [-1, 51, 8, 8]             102\n",
      "             ReLU-20             [-1, 51, 8, 8]               0\n",
      "        MaxPool2d-21             [-1, 51, 4, 4]               0\n",
      "           Conv2d-22             [-1, 51, 4, 4]          23,409\n",
      "      BatchNorm2d-23             [-1, 51, 4, 4]             102\n",
      "             ReLU-24             [-1, 51, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]         235,008\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-28            [-1, 512, 2, 2]               0\n",
      "           Linear-29                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 310,352\n",
      "Trainable params: 310,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.17\n",
      "Params size (MB): 1.18\n",
      "Estimated Total Size (MB): 2.36\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-2           [-1, 32, 16, 16]              64\n",
      "             ReLU6-3           [-1, 32, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-5           [-1, 32, 16, 16]              64\n",
      "             ReLU6-6           [-1, 32, 16, 16]               0\n",
      "            Conv2d-7           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-8           [-1, 16, 16, 16]              32\n",
      "  InvertedResidual-9           [-1, 16, 16, 16]               0\n",
      "           Conv2d-10           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 16, 16]             192\n",
      "            ReLU6-12           [-1, 96, 16, 16]               0\n",
      "           Conv2d-13             [-1, 96, 8, 8]             864\n",
      "      BatchNorm2d-14             [-1, 96, 8, 8]             192\n",
      "            ReLU6-15             [-1, 96, 8, 8]               0\n",
      "           Conv2d-16             [-1, 24, 8, 8]           2,304\n",
      "      BatchNorm2d-17             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-18             [-1, 24, 8, 8]               0\n",
      "           Conv2d-19            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-20            [-1, 144, 8, 8]             288\n",
      "            ReLU6-21            [-1, 144, 8, 8]               0\n",
      "           Conv2d-22            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-23            [-1, 144, 8, 8]             288\n",
      "            ReLU6-24            [-1, 144, 8, 8]               0\n",
      "           Conv2d-25             [-1, 24, 8, 8]           3,456\n",
      "      BatchNorm2d-26             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-27             [-1, 24, 8, 8]               0\n",
      "           Conv2d-28            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-29            [-1, 144, 8, 8]             288\n",
      "            ReLU6-30            [-1, 144, 8, 8]               0\n",
      "           Conv2d-31            [-1, 144, 4, 4]           1,296\n",
      "      BatchNorm2d-32            [-1, 144, 4, 4]             288\n",
      "            ReLU6-33            [-1, 144, 4, 4]               0\n",
      "           Conv2d-34             [-1, 32, 4, 4]           4,608\n",
      "      BatchNorm2d-35             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-36             [-1, 32, 4, 4]               0\n",
      "           Conv2d-37            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-38            [-1, 192, 4, 4]             384\n",
      "            ReLU6-39            [-1, 192, 4, 4]               0\n",
      "           Conv2d-40            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-41            [-1, 192, 4, 4]             384\n",
      "            ReLU6-42            [-1, 192, 4, 4]               0\n",
      "           Conv2d-43             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-44             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-45             [-1, 32, 4, 4]               0\n",
      "           Conv2d-46            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-47            [-1, 192, 4, 4]             384\n",
      "            ReLU6-48            [-1, 192, 4, 4]               0\n",
      "           Conv2d-49            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-50            [-1, 192, 4, 4]             384\n",
      "            ReLU6-51            [-1, 192, 4, 4]               0\n",
      "           Conv2d-52             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-53             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-54             [-1, 32, 4, 4]               0\n",
      "           Conv2d-55            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-56            [-1, 192, 4, 4]             384\n",
      "            ReLU6-57            [-1, 192, 4, 4]               0\n",
      "           Conv2d-58            [-1, 192, 2, 2]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 2, 2]             384\n",
      "            ReLU6-60            [-1, 192, 2, 2]               0\n",
      "           Conv2d-61             [-1, 64, 2, 2]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-63             [-1, 64, 2, 2]               0\n",
      "           Conv2d-64            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 2, 2]             768\n",
      "            ReLU6-66            [-1, 384, 2, 2]               0\n",
      "           Conv2d-67            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 2, 2]             768\n",
      "            ReLU6-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-72             [-1, 64, 2, 2]               0\n",
      "           Conv2d-73            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 2, 2]             768\n",
      "            ReLU6-75            [-1, 384, 2, 2]               0\n",
      "           Conv2d-76            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 2, 2]             768\n",
      "            ReLU6-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-81             [-1, 64, 2, 2]               0\n",
      "           Conv2d-82            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 2, 2]             768\n",
      "            ReLU6-84            [-1, 384, 2, 2]               0\n",
      "           Conv2d-85            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 2, 2]             768\n",
      "            ReLU6-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-90             [-1, 64, 2, 2]               0\n",
      "           Conv2d-91            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 2, 2]             768\n",
      "            ReLU6-93            [-1, 384, 2, 2]               0\n",
      "           Conv2d-94            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 2, 2]             768\n",
      "            ReLU6-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97             [-1, 96, 2, 2]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 2, 2]             192\n",
      " InvertedResidual-99             [-1, 96, 2, 2]               0\n",
      "          Conv2d-100            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-102            [-1, 576, 2, 2]               0\n",
      "          Conv2d-103            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-105            [-1, 576, 2, 2]               0\n",
      "          Conv2d-106             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-108             [-1, 96, 2, 2]               0\n",
      "          Conv2d-109            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-111            [-1, 576, 2, 2]               0\n",
      "          Conv2d-112            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-114            [-1, 576, 2, 2]               0\n",
      "          Conv2d-115             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-117             [-1, 96, 2, 2]               0\n",
      "          Conv2d-118            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-120            [-1, 576, 2, 2]               0\n",
      "          Conv2d-121            [-1, 576, 1, 1]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 1, 1]           1,152\n",
      "           ReLU6-123            [-1, 576, 1, 1]               0\n",
      "          Conv2d-124            [-1, 160, 1, 1]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-126            [-1, 160, 1, 1]               0\n",
      "          Conv2d-127            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-129            [-1, 960, 1, 1]               0\n",
      "          Conv2d-130            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-132            [-1, 960, 1, 1]               0\n",
      "          Conv2d-133            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-135            [-1, 160, 1, 1]               0\n",
      "          Conv2d-136            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-138            [-1, 960, 1, 1]               0\n",
      "          Conv2d-139            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-141            [-1, 960, 1, 1]               0\n",
      "          Conv2d-142            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-144            [-1, 160, 1, 1]               0\n",
      "          Conv2d-145            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-147            [-1, 960, 1, 1]               0\n",
      "          Conv2d-148            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-150            [-1, 960, 1, 1]               0\n",
      "          Conv2d-151            [-1, 320, 1, 1]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 1, 1]             640\n",
      "InvertedResidual-153            [-1, 320, 1, 1]               0\n",
      "          Conv2d-154           [-1, 1280, 1, 1]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 1, 1]           2,560\n",
      "           ReLU6-156           [-1, 1280, 1, 1]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 3,504,872\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.14\n",
      "Params size (MB): 13.37\n",
      "Estimated Total Size (MB): 16.52\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 3, 16, 16]              81\n",
      "       BatchNorm2d-2            [-1, 3, 16, 16]               6\n",
      "             ReLU6-3            [-1, 3, 16, 16]               0\n",
      "            Conv2d-4            [-1, 3, 16, 16]              27\n",
      "       BatchNorm2d-5            [-1, 3, 16, 16]               6\n",
      "             ReLU6-6            [-1, 3, 16, 16]               0\n",
      "            Conv2d-7            [-1, 2, 16, 16]               6\n",
      "       BatchNorm2d-8            [-1, 2, 16, 16]               4\n",
      "  InvertedResidual-9            [-1, 2, 16, 16]               0\n",
      "           Conv2d-10           [-1, 10, 16, 16]              20\n",
      "      BatchNorm2d-11           [-1, 10, 16, 16]              20\n",
      "            ReLU6-12           [-1, 10, 16, 16]               0\n",
      "           Conv2d-13             [-1, 10, 8, 8]              90\n",
      "      BatchNorm2d-14             [-1, 10, 8, 8]              20\n",
      "            ReLU6-15             [-1, 10, 8, 8]               0\n",
      "           Conv2d-16              [-1, 2, 8, 8]              20\n",
      "      BatchNorm2d-17              [-1, 2, 8, 8]               4\n",
      " InvertedResidual-18              [-1, 2, 8, 8]               0\n",
      "           Conv2d-19             [-1, 14, 8, 8]              28\n",
      "      BatchNorm2d-20             [-1, 14, 8, 8]              28\n",
      "            ReLU6-21             [-1, 14, 8, 8]               0\n",
      "           Conv2d-22             [-1, 14, 8, 8]             126\n",
      "      BatchNorm2d-23             [-1, 14, 8, 8]              28\n",
      "            ReLU6-24             [-1, 14, 8, 8]               0\n",
      "           Conv2d-25              [-1, 2, 8, 8]              28\n",
      "      BatchNorm2d-26              [-1, 2, 8, 8]               4\n",
      " InvertedResidual-27              [-1, 2, 8, 8]               0\n",
      "           Conv2d-28             [-1, 14, 8, 8]              28\n",
      "      BatchNorm2d-29             [-1, 14, 8, 8]              28\n",
      "            ReLU6-30             [-1, 14, 8, 8]               0\n",
      "           Conv2d-31             [-1, 14, 4, 4]             126\n",
      "      BatchNorm2d-32             [-1, 14, 4, 4]              28\n",
      "            ReLU6-33             [-1, 14, 4, 4]               0\n",
      "           Conv2d-34              [-1, 3, 4, 4]              42\n",
      "      BatchNorm2d-35              [-1, 3, 4, 4]               6\n",
      " InvertedResidual-36              [-1, 3, 4, 4]               0\n",
      "           Conv2d-37             [-1, 19, 4, 4]              57\n",
      "      BatchNorm2d-38             [-1, 19, 4, 4]              38\n",
      "            ReLU6-39             [-1, 19, 4, 4]               0\n",
      "           Conv2d-40             [-1, 19, 4, 4]             171\n",
      "      BatchNorm2d-41             [-1, 19, 4, 4]              38\n",
      "            ReLU6-42             [-1, 19, 4, 4]               0\n",
      "           Conv2d-43              [-1, 3, 4, 4]              57\n",
      "      BatchNorm2d-44              [-1, 3, 4, 4]               6\n",
      " InvertedResidual-45              [-1, 3, 4, 4]               0\n",
      "           Conv2d-46             [-1, 19, 4, 4]              57\n",
      "      BatchNorm2d-47             [-1, 19, 4, 4]              38\n",
      "            ReLU6-48             [-1, 19, 4, 4]               0\n",
      "           Conv2d-49             [-1, 19, 4, 4]             171\n",
      "      BatchNorm2d-50             [-1, 19, 4, 4]              38\n",
      "            ReLU6-51             [-1, 19, 4, 4]               0\n",
      "           Conv2d-52              [-1, 3, 4, 4]              57\n",
      "      BatchNorm2d-53              [-1, 3, 4, 4]               6\n",
      " InvertedResidual-54              [-1, 3, 4, 4]               0\n",
      "           Conv2d-55             [-1, 19, 4, 4]              57\n",
      "      BatchNorm2d-56             [-1, 19, 4, 4]              38\n",
      "            ReLU6-57             [-1, 19, 4, 4]               0\n",
      "           Conv2d-58             [-1, 19, 2, 2]             171\n",
      "      BatchNorm2d-59             [-1, 19, 2, 2]              38\n",
      "            ReLU6-60             [-1, 19, 2, 2]               0\n",
      "           Conv2d-61              [-1, 6, 2, 2]             114\n",
      "      BatchNorm2d-62              [-1, 6, 2, 2]              12\n",
      " InvertedResidual-63              [-1, 6, 2, 2]               0\n",
      "           Conv2d-64             [-1, 38, 2, 2]             228\n",
      "      BatchNorm2d-65             [-1, 38, 2, 2]              76\n",
      "            ReLU6-66             [-1, 38, 2, 2]               0\n",
      "           Conv2d-67             [-1, 38, 2, 2]             342\n",
      "      BatchNorm2d-68             [-1, 38, 2, 2]              76\n",
      "            ReLU6-69             [-1, 38, 2, 2]               0\n",
      "           Conv2d-70              [-1, 6, 2, 2]             228\n",
      "      BatchNorm2d-71              [-1, 6, 2, 2]              12\n",
      " InvertedResidual-72              [-1, 6, 2, 2]               0\n",
      "           Conv2d-73             [-1, 38, 2, 2]             228\n",
      "      BatchNorm2d-74             [-1, 38, 2, 2]              76\n",
      "            ReLU6-75             [-1, 38, 2, 2]               0\n",
      "           Conv2d-76             [-1, 38, 2, 2]             342\n",
      "      BatchNorm2d-77             [-1, 38, 2, 2]              76\n",
      "            ReLU6-78             [-1, 38, 2, 2]               0\n",
      "           Conv2d-79              [-1, 6, 2, 2]             228\n",
      "      BatchNorm2d-80              [-1, 6, 2, 2]              12\n",
      " InvertedResidual-81              [-1, 6, 2, 2]               0\n",
      "           Conv2d-82             [-1, 38, 2, 2]             228\n",
      "      BatchNorm2d-83             [-1, 38, 2, 2]              76\n",
      "            ReLU6-84             [-1, 38, 2, 2]               0\n",
      "           Conv2d-85             [-1, 38, 2, 2]             342\n",
      "      BatchNorm2d-86             [-1, 38, 2, 2]              76\n",
      "            ReLU6-87             [-1, 38, 2, 2]               0\n",
      "           Conv2d-88              [-1, 6, 2, 2]             228\n",
      "      BatchNorm2d-89              [-1, 6, 2, 2]              12\n",
      " InvertedResidual-90              [-1, 6, 2, 2]               0\n",
      "           Conv2d-91             [-1, 38, 2, 2]             228\n",
      "      BatchNorm2d-92             [-1, 38, 2, 2]              76\n",
      "            ReLU6-93             [-1, 38, 2, 2]               0\n",
      "           Conv2d-94             [-1, 38, 2, 2]             342\n",
      "      BatchNorm2d-95             [-1, 38, 2, 2]              76\n",
      "            ReLU6-96             [-1, 38, 2, 2]               0\n",
      "           Conv2d-97             [-1, 10, 2, 2]             380\n",
      "      BatchNorm2d-98             [-1, 10, 2, 2]              20\n",
      " InvertedResidual-99             [-1, 10, 2, 2]               0\n",
      "          Conv2d-100             [-1, 58, 2, 2]             580\n",
      "     BatchNorm2d-101             [-1, 58, 2, 2]             116\n",
      "           ReLU6-102             [-1, 58, 2, 2]               0\n",
      "          Conv2d-103             [-1, 58, 2, 2]             522\n",
      "     BatchNorm2d-104             [-1, 58, 2, 2]             116\n",
      "           ReLU6-105             [-1, 58, 2, 2]               0\n",
      "          Conv2d-106             [-1, 10, 2, 2]             580\n",
      "     BatchNorm2d-107             [-1, 10, 2, 2]              20\n",
      "InvertedResidual-108             [-1, 10, 2, 2]               0\n",
      "          Conv2d-109             [-1, 58, 2, 2]             580\n",
      "     BatchNorm2d-110             [-1, 58, 2, 2]             116\n",
      "           ReLU6-111             [-1, 58, 2, 2]               0\n",
      "          Conv2d-112             [-1, 58, 2, 2]             522\n",
      "     BatchNorm2d-113             [-1, 58, 2, 2]             116\n",
      "           ReLU6-114             [-1, 58, 2, 2]               0\n",
      "          Conv2d-115             [-1, 10, 2, 2]             580\n",
      "     BatchNorm2d-116             [-1, 10, 2, 2]              20\n",
      "InvertedResidual-117             [-1, 10, 2, 2]               0\n",
      "          Conv2d-118             [-1, 58, 2, 2]             580\n",
      "     BatchNorm2d-119             [-1, 58, 2, 2]             116\n",
      "           ReLU6-120             [-1, 58, 2, 2]               0\n",
      "          Conv2d-121             [-1, 58, 1, 1]             522\n",
      "     BatchNorm2d-122             [-1, 58, 1, 1]             116\n",
      "           ReLU6-123             [-1, 58, 1, 1]               0\n",
      "          Conv2d-124             [-1, 16, 1, 1]             928\n",
      "     BatchNorm2d-125             [-1, 16, 1, 1]              32\n",
      "InvertedResidual-126             [-1, 16, 1, 1]               0\n",
      "          Conv2d-127             [-1, 96, 1, 1]           1,536\n",
      "     BatchNorm2d-128             [-1, 96, 1, 1]             192\n",
      "           ReLU6-129             [-1, 96, 1, 1]               0\n",
      "          Conv2d-130             [-1, 96, 1, 1]             864\n",
      "     BatchNorm2d-131             [-1, 96, 1, 1]             192\n",
      "           ReLU6-132             [-1, 96, 1, 1]               0\n",
      "          Conv2d-133             [-1, 16, 1, 1]           1,536\n",
      "     BatchNorm2d-134             [-1, 16, 1, 1]              32\n",
      "InvertedResidual-135             [-1, 16, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]           1,536\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "           ReLU6-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139             [-1, 96, 1, 1]             864\n",
      "     BatchNorm2d-140             [-1, 96, 1, 1]             192\n",
      "           ReLU6-141             [-1, 96, 1, 1]               0\n",
      "          Conv2d-142             [-1, 16, 1, 1]           1,536\n",
      "     BatchNorm2d-143             [-1, 16, 1, 1]              32\n",
      "InvertedResidual-144             [-1, 16, 1, 1]               0\n",
      "          Conv2d-145             [-1, 96, 1, 1]           1,536\n",
      "     BatchNorm2d-146             [-1, 96, 1, 1]             192\n",
      "           ReLU6-147             [-1, 96, 1, 1]               0\n",
      "          Conv2d-148             [-1, 96, 1, 1]             864\n",
      "     BatchNorm2d-149             [-1, 96, 1, 1]             192\n",
      "           ReLU6-150             [-1, 96, 1, 1]               0\n",
      "          Conv2d-151             [-1, 32, 1, 1]           3,072\n",
      "     BatchNorm2d-152             [-1, 32, 1, 1]              64\n",
      "InvertedResidual-153             [-1, 32, 1, 1]               0\n",
      "          Conv2d-154           [-1, 1280, 1, 1]          40,960\n",
      "     BatchNorm2d-155           [-1, 1280, 1, 1]           2,560\n",
      "           ReLU6-156           [-1, 1280, 1, 1]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 1,351,282\n",
      "Trainable params: 1,351,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 5.15\n",
      "Estimated Total Size (MB): 5.52\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 16, 16]             432\n",
      "       BatchNorm2d-2           [-1, 16, 16, 16]              32\n",
      "         Hardswish-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4             [-1, 16, 8, 8]             144\n",
      "       BatchNorm2d-5             [-1, 16, 8, 8]              32\n",
      "              ReLU-6             [-1, 16, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12             [-1, 16, 8, 8]               0\n",
      "           Conv2d-13             [-1, 16, 8, 8]             256\n",
      "      BatchNorm2d-14             [-1, 16, 8, 8]              32\n",
      " InvertedResidual-15             [-1, 16, 8, 8]               0\n",
      "           Conv2d-16             [-1, 72, 8, 8]           1,152\n",
      "      BatchNorm2d-17             [-1, 72, 8, 8]             144\n",
      "             ReLU-18             [-1, 72, 8, 8]               0\n",
      "           Conv2d-19             [-1, 72, 4, 4]             648\n",
      "      BatchNorm2d-20             [-1, 72, 4, 4]             144\n",
      "             ReLU-21             [-1, 72, 4, 4]               0\n",
      "           Conv2d-22             [-1, 24, 4, 4]           1,728\n",
      "      BatchNorm2d-23             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-24             [-1, 24, 4, 4]               0\n",
      "           Conv2d-25             [-1, 88, 4, 4]           2,112\n",
      "      BatchNorm2d-26             [-1, 88, 4, 4]             176\n",
      "             ReLU-27             [-1, 88, 4, 4]               0\n",
      "           Conv2d-28             [-1, 88, 4, 4]             792\n",
      "      BatchNorm2d-29             [-1, 88, 4, 4]             176\n",
      "             ReLU-30             [-1, 88, 4, 4]               0\n",
      "           Conv2d-31             [-1, 24, 4, 4]           2,112\n",
      "      BatchNorm2d-32             [-1, 24, 4, 4]              48\n",
      " InvertedResidual-33             [-1, 24, 4, 4]               0\n",
      "           Conv2d-34             [-1, 96, 4, 4]           2,304\n",
      "      BatchNorm2d-35             [-1, 96, 4, 4]             192\n",
      "        Hardswish-36             [-1, 96, 4, 4]               0\n",
      "           Conv2d-37             [-1, 96, 2, 2]           2,400\n",
      "      BatchNorm2d-38             [-1, 96, 2, 2]             192\n",
      "        Hardswish-39             [-1, 96, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 96, 2, 2]               0\n",
      "           Conv2d-46             [-1, 40, 2, 2]           3,840\n",
      "      BatchNorm2d-47             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-48             [-1, 40, 2, 2]               0\n",
      "           Conv2d-49            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-50            [-1, 240, 2, 2]             480\n",
      "        Hardswish-51            [-1, 240, 2, 2]               0\n",
      "           Conv2d-52            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-53            [-1, 240, 2, 2]             480\n",
      "        Hardswish-54            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60            [-1, 240, 2, 2]               0\n",
      "           Conv2d-61             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-62             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-63             [-1, 40, 2, 2]               0\n",
      "           Conv2d-64            [-1, 240, 2, 2]           9,600\n",
      "      BatchNorm2d-65            [-1, 240, 2, 2]             480\n",
      "        Hardswish-66            [-1, 240, 2, 2]               0\n",
      "           Conv2d-67            [-1, 240, 2, 2]           6,000\n",
      "      BatchNorm2d-68            [-1, 240, 2, 2]             480\n",
      "        Hardswish-69            [-1, 240, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75            [-1, 240, 2, 2]               0\n",
      "           Conv2d-76             [-1, 40, 2, 2]           9,600\n",
      "      BatchNorm2d-77             [-1, 40, 2, 2]              80\n",
      " InvertedResidual-78             [-1, 40, 2, 2]               0\n",
      "           Conv2d-79            [-1, 120, 2, 2]           4,800\n",
      "      BatchNorm2d-80            [-1, 120, 2, 2]             240\n",
      "        Hardswish-81            [-1, 120, 2, 2]               0\n",
      "           Conv2d-82            [-1, 120, 2, 2]           3,000\n",
      "      BatchNorm2d-83            [-1, 120, 2, 2]             240\n",
      "        Hardswish-84            [-1, 120, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90            [-1, 120, 2, 2]               0\n",
      "           Conv2d-91             [-1, 48, 2, 2]           5,760\n",
      "      BatchNorm2d-92             [-1, 48, 2, 2]              96\n",
      " InvertedResidual-93             [-1, 48, 2, 2]               0\n",
      "           Conv2d-94            [-1, 144, 2, 2]           6,912\n",
      "      BatchNorm2d-95            [-1, 144, 2, 2]             288\n",
      "        Hardswish-96            [-1, 144, 2, 2]               0\n",
      "           Conv2d-97            [-1, 144, 2, 2]           3,600\n",
      "      BatchNorm2d-98            [-1, 144, 2, 2]             288\n",
      "        Hardswish-99            [-1, 144, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105            [-1, 144, 2, 2]               0\n",
      "          Conv2d-106             [-1, 48, 2, 2]           6,912\n",
      "     BatchNorm2d-107             [-1, 48, 2, 2]              96\n",
      "InvertedResidual-108             [-1, 48, 2, 2]               0\n",
      "          Conv2d-109            [-1, 288, 2, 2]          13,824\n",
      "     BatchNorm2d-110            [-1, 288, 2, 2]             576\n",
      "       Hardswish-111            [-1, 288, 2, 2]               0\n",
      "          Conv2d-112            [-1, 288, 1, 1]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 1, 1]             576\n",
      "       Hardswish-114            [-1, 288, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 1, 1]               0\n",
      "          Conv2d-121             [-1, 96, 1, 1]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-123             [-1, 96, 1, 1]               0\n",
      "          Conv2d-124            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-126            [-1, 576, 1, 1]               0\n",
      "          Conv2d-127            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-129            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 1, 1]               0\n",
      "          Conv2d-136             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-138             [-1, 96, 1, 1]               0\n",
      "          Conv2d-139            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-141            [-1, 576, 1, 1]               0\n",
      "          Conv2d-142            [-1, 576, 1, 1]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-144            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 1, 1]               0\n",
      "          Conv2d-151             [-1, 96, 1, 1]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 1, 1]             192\n",
      "InvertedResidual-153             [-1, 96, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 2,542,856\n",
      "Trainable params: 2,542,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 9.70\n",
      "Estimated Total Size (MB): 10.51\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 2, 16, 16]              54\n",
      "       BatchNorm2d-2            [-1, 2, 16, 16]               4\n",
      "         Hardswish-3            [-1, 2, 16, 16]               0\n",
      "            Conv2d-4              [-1, 2, 8, 8]              18\n",
      "       BatchNorm2d-5              [-1, 2, 8, 8]               4\n",
      "              ReLU-6              [-1, 2, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 2, 1, 1]               0\n",
      "            Conv2d-8              [-1, 1, 1, 1]               3\n",
      "              ReLU-9              [-1, 1, 1, 1]               0\n",
      "           Conv2d-10              [-1, 2, 1, 1]               4\n",
      "      Hardsigmoid-11              [-1, 2, 1, 1]               0\n",
      "SqueezeExcitation-12              [-1, 2, 8, 8]               0\n",
      "           Conv2d-13              [-1, 2, 8, 8]               4\n",
      "      BatchNorm2d-14              [-1, 2, 8, 8]               4\n",
      " InvertedResidual-15              [-1, 2, 8, 8]               0\n",
      "           Conv2d-16              [-1, 7, 8, 8]              14\n",
      "      BatchNorm2d-17              [-1, 7, 8, 8]              14\n",
      "             ReLU-18              [-1, 7, 8, 8]               0\n",
      "           Conv2d-19              [-1, 7, 4, 4]              63\n",
      "      BatchNorm2d-20              [-1, 7, 4, 4]              14\n",
      "             ReLU-21              [-1, 7, 4, 4]               0\n",
      "           Conv2d-22              [-1, 2, 4, 4]              14\n",
      "      BatchNorm2d-23              [-1, 2, 4, 4]               4\n",
      " InvertedResidual-24              [-1, 2, 4, 4]               0\n",
      "           Conv2d-25              [-1, 9, 4, 4]              18\n",
      "      BatchNorm2d-26              [-1, 9, 4, 4]              18\n",
      "             ReLU-27              [-1, 9, 4, 4]               0\n",
      "           Conv2d-28              [-1, 9, 4, 4]              81\n",
      "      BatchNorm2d-29              [-1, 9, 4, 4]              18\n",
      "             ReLU-30              [-1, 9, 4, 4]               0\n",
      "           Conv2d-31              [-1, 2, 4, 4]              18\n",
      "      BatchNorm2d-32              [-1, 2, 4, 4]               4\n",
      " InvertedResidual-33              [-1, 2, 4, 4]               0\n",
      "           Conv2d-34             [-1, 10, 4, 4]              20\n",
      "      BatchNorm2d-35             [-1, 10, 4, 4]              20\n",
      "        Hardswish-36             [-1, 10, 4, 4]               0\n",
      "           Conv2d-37             [-1, 10, 2, 2]             250\n",
      "      BatchNorm2d-38             [-1, 10, 2, 2]              20\n",
      "        Hardswish-39             [-1, 10, 2, 2]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 10, 1, 1]               0\n",
      "           Conv2d-41              [-1, 2, 1, 1]              22\n",
      "             ReLU-42              [-1, 2, 1, 1]               0\n",
      "           Conv2d-43             [-1, 10, 1, 1]              30\n",
      "      Hardsigmoid-44             [-1, 10, 1, 1]               0\n",
      "SqueezeExcitation-45             [-1, 10, 2, 2]               0\n",
      "           Conv2d-46              [-1, 4, 2, 2]              40\n",
      "      BatchNorm2d-47              [-1, 4, 2, 2]               8\n",
      " InvertedResidual-48              [-1, 4, 2, 2]               0\n",
      "           Conv2d-49             [-1, 24, 2, 2]              96\n",
      "      BatchNorm2d-50             [-1, 24, 2, 2]              48\n",
      "        Hardswish-51             [-1, 24, 2, 2]               0\n",
      "           Conv2d-52             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-53             [-1, 24, 2, 2]              48\n",
      "        Hardswish-54             [-1, 24, 2, 2]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 24, 1, 1]               0\n",
      "           Conv2d-56              [-1, 6, 1, 1]             150\n",
      "             ReLU-57              [-1, 6, 1, 1]               0\n",
      "           Conv2d-58             [-1, 24, 1, 1]             168\n",
      "      Hardsigmoid-59             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-60             [-1, 24, 2, 2]               0\n",
      "           Conv2d-61              [-1, 4, 2, 2]              96\n",
      "      BatchNorm2d-62              [-1, 4, 2, 2]               8\n",
      " InvertedResidual-63              [-1, 4, 2, 2]               0\n",
      "           Conv2d-64             [-1, 24, 2, 2]              96\n",
      "      BatchNorm2d-65             [-1, 24, 2, 2]              48\n",
      "        Hardswish-66             [-1, 24, 2, 2]               0\n",
      "           Conv2d-67             [-1, 24, 2, 2]             600\n",
      "      BatchNorm2d-68             [-1, 24, 2, 2]              48\n",
      "        Hardswish-69             [-1, 24, 2, 2]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 24, 1, 1]               0\n",
      "           Conv2d-71              [-1, 6, 1, 1]             150\n",
      "             ReLU-72              [-1, 6, 1, 1]               0\n",
      "           Conv2d-73             [-1, 24, 1, 1]             168\n",
      "      Hardsigmoid-74             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-75             [-1, 24, 2, 2]               0\n",
      "           Conv2d-76              [-1, 4, 2, 2]              96\n",
      "      BatchNorm2d-77              [-1, 4, 2, 2]               8\n",
      " InvertedResidual-78              [-1, 4, 2, 2]               0\n",
      "           Conv2d-79             [-1, 12, 2, 2]              48\n",
      "      BatchNorm2d-80             [-1, 12, 2, 2]              24\n",
      "        Hardswish-81             [-1, 12, 2, 2]               0\n",
      "           Conv2d-82             [-1, 12, 2, 2]             300\n",
      "      BatchNorm2d-83             [-1, 12, 2, 2]              24\n",
      "        Hardswish-84             [-1, 12, 2, 2]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 12, 1, 1]               0\n",
      "           Conv2d-86              [-1, 3, 1, 1]              39\n",
      "             ReLU-87              [-1, 3, 1, 1]               0\n",
      "           Conv2d-88             [-1, 12, 1, 1]              48\n",
      "      Hardsigmoid-89             [-1, 12, 1, 1]               0\n",
      "SqueezeExcitation-90             [-1, 12, 2, 2]               0\n",
      "           Conv2d-91              [-1, 5, 2, 2]              60\n",
      "      BatchNorm2d-92              [-1, 5, 2, 2]              10\n",
      " InvertedResidual-93              [-1, 5, 2, 2]               0\n",
      "           Conv2d-94             [-1, 14, 2, 2]              70\n",
      "      BatchNorm2d-95             [-1, 14, 2, 2]              28\n",
      "        Hardswish-96             [-1, 14, 2, 2]               0\n",
      "           Conv2d-97             [-1, 14, 2, 2]             350\n",
      "      BatchNorm2d-98             [-1, 14, 2, 2]              28\n",
      "        Hardswish-99             [-1, 14, 2, 2]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 14, 1, 1]               0\n",
      "          Conv2d-101              [-1, 4, 1, 1]              60\n",
      "            ReLU-102              [-1, 4, 1, 1]               0\n",
      "          Conv2d-103             [-1, 14, 1, 1]              70\n",
      "     Hardsigmoid-104             [-1, 14, 1, 1]               0\n",
      "SqueezeExcitation-105             [-1, 14, 2, 2]               0\n",
      "          Conv2d-106              [-1, 5, 2, 2]              70\n",
      "     BatchNorm2d-107              [-1, 5, 2, 2]              10\n",
      "InvertedResidual-108              [-1, 5, 2, 2]               0\n",
      "          Conv2d-109             [-1, 29, 2, 2]             145\n",
      "     BatchNorm2d-110             [-1, 29, 2, 2]              58\n",
      "       Hardswish-111             [-1, 29, 2, 2]               0\n",
      "          Conv2d-112             [-1, 29, 1, 1]             725\n",
      "     BatchNorm2d-113             [-1, 29, 1, 1]              58\n",
      "       Hardswish-114             [-1, 29, 1, 1]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 29, 1, 1]               0\n",
      "          Conv2d-116              [-1, 7, 1, 1]             210\n",
      "            ReLU-117              [-1, 7, 1, 1]               0\n",
      "          Conv2d-118             [-1, 29, 1, 1]             232\n",
      "     Hardsigmoid-119             [-1, 29, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 29, 1, 1]               0\n",
      "          Conv2d-121             [-1, 10, 1, 1]             290\n",
      "     BatchNorm2d-122             [-1, 10, 1, 1]              20\n",
      "InvertedResidual-123             [-1, 10, 1, 1]               0\n",
      "          Conv2d-124             [-1, 58, 1, 1]             580\n",
      "     BatchNorm2d-125             [-1, 58, 1, 1]             116\n",
      "       Hardswish-126             [-1, 58, 1, 1]               0\n",
      "          Conv2d-127             [-1, 58, 1, 1]           1,450\n",
      "     BatchNorm2d-128             [-1, 58, 1, 1]             116\n",
      "       Hardswish-129             [-1, 58, 1, 1]               0\n",
      "AdaptiveAvgPool2d-130             [-1, 58, 1, 1]               0\n",
      "          Conv2d-131             [-1, 14, 1, 1]             826\n",
      "            ReLU-132             [-1, 14, 1, 1]               0\n",
      "          Conv2d-133             [-1, 58, 1, 1]             870\n",
      "     Hardsigmoid-134             [-1, 58, 1, 1]               0\n",
      "SqueezeExcitation-135             [-1, 58, 1, 1]               0\n",
      "          Conv2d-136             [-1, 10, 1, 1]             580\n",
      "     BatchNorm2d-137             [-1, 10, 1, 1]              20\n",
      "InvertedResidual-138             [-1, 10, 1, 1]               0\n",
      "          Conv2d-139             [-1, 58, 1, 1]             580\n",
      "     BatchNorm2d-140             [-1, 58, 1, 1]             116\n",
      "       Hardswish-141             [-1, 58, 1, 1]               0\n",
      "          Conv2d-142             [-1, 58, 1, 1]           1,450\n",
      "     BatchNorm2d-143             [-1, 58, 1, 1]             116\n",
      "       Hardswish-144             [-1, 58, 1, 1]               0\n",
      "AdaptiveAvgPool2d-145             [-1, 58, 1, 1]               0\n",
      "          Conv2d-146             [-1, 14, 1, 1]             826\n",
      "            ReLU-147             [-1, 14, 1, 1]               0\n",
      "          Conv2d-148             [-1, 58, 1, 1]             870\n",
      "     Hardsigmoid-149             [-1, 58, 1, 1]               0\n",
      "SqueezeExcitation-150             [-1, 58, 1, 1]               0\n",
      "          Conv2d-151             [-1, 10, 1, 1]             580\n",
      "     BatchNorm2d-152             [-1, 10, 1, 1]              20\n",
      "InvertedResidual-153             [-1, 10, 1, 1]               0\n",
      "          Conv2d-154            [-1, 576, 1, 1]           5,760\n",
      "     BatchNorm2d-155            [-1, 576, 1, 1]           1,152\n",
      "       Hardswish-156            [-1, 576, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "          Linear-158                 [-1, 1024]         590,848\n",
      "       Hardswish-159                 [-1, 1024]               0\n",
      "         Dropout-160                 [-1, 1024]               0\n",
      "          Linear-161                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 1,638,066\n",
      "Trainable params: 1,638,066\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 6.25\n",
      "Estimated Total Size (MB): 6.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model = mobilenet_v3\n",
    "\n",
    "for model in model_list:\n",
    "    \n",
    "    mapped_layers = layer_mapping(model)\n",
    "    name_list = mapped_layers['name_type_shape'][:, 0]\n",
    "\n",
    "    possible_indices_ranges = cwp_possible_layers(name_list)\n",
    "    possible_indices_ranges = [lst for lst in possible_indices_ranges if len(lst) > 1]\n",
    "    \n",
    "    pruning_ratio = 0.90\n",
    "    pruning_layer_length = len(possible_indices_ranges)\n",
    "    pruning_ratio_list = (pruning_layer_length) * [pruning_ratio]\n",
    "    \n",
    "    pruned_model, original_model = prune_cwp(model,pruning_ratio_list)\n",
    "    summary(original_model,(3,32,32))\n",
    "    summary(pruned_model,(3,32,32))\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
