{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEo_wyhLlHs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5imcpcgZLlHt"
      },
      "source": [
        "===============================\\\n",
        "   Compression Pipeline\\\n",
        "==============================="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3.11-distutils\n",
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11Mh1CbWTdOB",
        "outputId": "b577143c-b416-41b7-e01a-060495ae8682"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.11-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o8dmwh_LvSZ",
        "outputId": "67236f90-b2dc-438f-a9c3-633ecd1c3ac8"
      },
      "source": [
        "%pip install -q git+https://github.com/satabios/sconce"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sconce (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-01T07:32:53.170470032Z",
          "start_time": "2023-12-01T07:32:50.766056935Z"
        },
        "id": "xEDfNQnMIW9H",
        "outputId": "150707a1-3f49-447b-adc9-23f786a28e8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import defaultdict, OrderedDict\n",
        "from sconce import sconce\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "import torch.optim as optim\n",
        "\n",
        "assert torch.cuda.is_available(), \\\n",
        "\"The current runtime does not have CUDA support.\" \\\n",
        "\"Please go to menu bar (Runtime - Change runtime type) and select GPU\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umyrKmgdEHQf"
      },
      "source": [
        "Load the Pre-Trained Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/satabios/pre-trained_cifar10/resolve/main/vgg.cifar.pretrained.pth?download=true -O vgg.cifar.pretrained.pth"
      ],
      "metadata": {
        "id": "BwjxH5Ne-my3",
        "outputId": "06a3ca8b-b6cf-46f8-b318-3d8eed3492c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-31 18:53:30--  https://huggingface.co/satabios/pre-trained_cifar10/resolve/main/vgg.cifar.pretrained.pth?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.97, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/55/fb/55fb4781756edef0ec8cbd6e84a7a05c9478cb5c1d83647b7e7d0a7b31bf7e7d/356e478a06348b2497dbaddddbfd09a4c454d6df7c9bcdf36e767b38a93ff43a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vgg.cifar.pretrained.pth%3B+filename%3D%22vgg.cifar.pretrained.pth%22%3B&Expires=1738353210&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODM1MzIxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU1L2ZiLzU1ZmI0NzgxNzU2ZWRlZjBlYzhjYmQ2ZTg0YTdhMDVjOTQ3OGNiNWMxZDgzNjQ3YjdlN2QwYTdiMzFiZjdlN2QvMzU2ZTQ3OGEwNjM0OGIyNDk3ZGJhZGRkZGJmZDA5YTRjNDU0ZDZkZjdjOWJjZGYzNmU3NjdiMzhhOTNmZjQzYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fdMFcbwOWuR3ytNZ5e8L%7E-04MMhnWYEoPLukRJVxWbNHBV1wsAfmS1YXeOvrY1emYO1SEzt%7EPWmDhaqkawXvr4C8cAXHaR5yNwPpfGipcsw14N8-vd9AMeBPIpE68otAK70LnR%7EzBjJQxMUIqZcCp4G8e1r7AZtgEILNGd%7EZI6rxgR56WyH1RMZfGokDZzyle2aPFm8WUYjiBQVnmeRl%7ErcJ4JP5Vu3SYiIo-3VARUxXNORTe8nkk18UIatVo8DZqAJjwQUx5UMRE9UJ6mGJcL9BterbRZOkWoWBKukbXqQN1c9DkU87VVKh56J6ruQlKGVEQzePPtBx2MkLUyDrOg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-01-31 18:53:30--  https://cdn-lfs-us-1.hf.co/repos/55/fb/55fb4781756edef0ec8cbd6e84a7a05c9478cb5c1d83647b7e7d0a7b31bf7e7d/356e478a06348b2497dbaddddbfd09a4c454d6df7c9bcdf36e767b38a93ff43a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vgg.cifar.pretrained.pth%3B+filename%3D%22vgg.cifar.pretrained.pth%22%3B&Expires=1738353210&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODM1MzIxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU1L2ZiLzU1ZmI0NzgxNzU2ZWRlZjBlYzhjYmQ2ZTg0YTdhMDVjOTQ3OGNiNWMxZDgzNjQ3YjdlN2QwYTdiMzFiZjdlN2QvMzU2ZTQ3OGEwNjM0OGIyNDk3ZGJhZGRkZGJmZDA5YTRjNDU0ZDZkZjdjOWJjZGYzNmU3NjdiMzhhOTNmZjQzYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fdMFcbwOWuR3ytNZ5e8L%7E-04MMhnWYEoPLukRJVxWbNHBV1wsAfmS1YXeOvrY1emYO1SEzt%7EPWmDhaqkawXvr4C8cAXHaR5yNwPpfGipcsw14N8-vd9AMeBPIpE68otAK70LnR%7EzBjJQxMUIqZcCp4G8e1r7AZtgEILNGd%7EZI6rxgR56WyH1RMZfGokDZzyle2aPFm8WUYjiBQVnmeRl%7ErcJ4JP5Vu3SYiIo-3VARUxXNORTe8nkk18UIatVo8DZqAJjwQUx5UMRE9UJ6mGJcL9BterbRZOkWoWBKukbXqQN1c9DkU87VVKh56J6ruQlKGVEQzePPtBx2MkLUyDrOg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.88.68, 13.33.88.42, 13.33.88.91, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.88.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36951795 (35M) [binary/octet-stream]\n",
            "Saving to: ‘vgg.cifar.pretrained.pth’\n",
            "\n",
            "vgg.cifar.pretraine 100%[===================>]  35.24M  14.9MB/s    in 2.4s    \n",
            "\n",
            "2025-01-31 18:53:33 (14.9 MB/s) - ‘vgg.cifar.pretrained.pth’ saved [36951795/36951795]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-01T07:32:56.646268058Z",
          "start_time": "2023-12-01T07:32:55.606697470Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8_S-wtTEGKu",
        "outputId": "a255bd95-93f4-4afe-df60-80081b060a24"
      },
      "source": [
        "\n",
        "class VGG(nn.Module):\n",
        "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    def add(name: str, layer: nn.Module) -> None:\n",
        "      layers.append((f\"{name}{counts[name]}\", layer))\n",
        "      counts[name] += 1\n",
        "\n",
        "    in_channels = 3\n",
        "    for x in self.ARCH:\n",
        "      if x != 'M':\n",
        "        # conv-bn-relu\n",
        "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
        "        add(\"bn\", nn.BatchNorm2d(x))\n",
        "        add(\"relu\", nn.ReLU(True))\n",
        "        in_channels = x\n",
        "      else:\n",
        "        # maxpool\n",
        "        add(\"pool\", nn.MaxPool2d(2))\n",
        "\n",
        "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
        "    self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
        "    x = self.backbone(x)\n",
        "\n",
        "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
        "    x = x.mean([2, 3])\n",
        "\n",
        "    # classifier: [N, 512] => [N, 10]\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "#load the pretrained model\n",
        "model_path = \"vgg.cifar.pretrained.pth\"\n",
        "\n",
        "model = VGG().cuda()\n",
        "checkpoint = torch.load(model_path, weights_only=True)\n",
        "model.load_state_dict(checkpoint)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o020nzh1Ea3s"
      },
      "source": [
        "Setup the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-01T07:33:50.919090166Z",
          "start_time": "2023-12-01T07:33:37.768089911Z"
        },
        "id": "nozAsRWpNfa8",
        "outputId": "99375954-aff9-422a-b61a-299c67abd960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "image_size = 32\n",
        "transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        RandomCrop(image_size, padding=4),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    \"test\": ToTensor(),\n",
        "}\n",
        "dataset = {}\n",
        "for split in [\"train\", \"test\"]:\n",
        "\n",
        "    dataset[split] = CIFAR10(\n",
        "    root=\"data/cifar10\",\n",
        "    train=(split == \"train\"),\n",
        "    download=True,\n",
        "    transform=transforms[split],\n",
        "    )\n",
        "\n",
        "dataloader = {}\n",
        "for split in ['train', 'test']:\n",
        "  dataloader[split] = DataLoader(\n",
        "    dataset[split],\n",
        "    batch_size=512,\n",
        "    shuffle=(split == 'train'),\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "  )\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar10/cifar-10-python.tar.gz to data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByJy8c5NCxD3"
      },
      "source": [
        "sconce Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA_wSOZe3whu"
      },
      "source": [
        "**Channel-Wise Pruning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vtPnGXwr4wka"
      },
      "source": [
        "sconces = sconce()\n",
        "sconces.model= copy.deepcopy(model)\n",
        "sconces.criterion = nn.CrossEntropyLoss() # Loss\n",
        "sconces.optimizer= optim.Adam(sconces.model.parameters(), lr=1e-4)\n",
        "sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)\n",
        "sconces.dataloader = dataloader\n",
        "sconces.epochs = 1 #Number of time we iterate over the data\n",
        "sconces.num_finetune_epochs = 1\n",
        "sconces.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sconces.experiment_name = \"vgg-cwp\"\n",
        "sconces.prune_mode = \"CWP\" # Supports Automated Pruning Ratio Detection\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-ZQHTA0L5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a11a85-1dbc-4b67-8b55-a2e7a786e3c9"
      },
      "source": [
        "# Compress the model Channel-Wise\n",
        "sconces.compress()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Dense Model Size Model=35.20 MiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Validation Accuracy: 93.13627254509018 %\n",
            "\n",
            " Channel-Wise Pruning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity Scan Time(mins): 6.284129293759664 \n",
            "\n",
            "Sparsity for each Layer:  dict_items([('backbone.conv0', 0.15000000000000002), ('backbone.conv1', 0.0), ('backbone.conv2', 0.0), ('backbone.conv3', 0.0), ('backbone.conv4', 0.0), ('backbone.conv5', 0.0), ('backbone.conv6', 0.1), ('backbone.conv7', 0.40000000000000013)])\n",
            "\n",
            "Pruning Time Consumed (mins): 0.0014287829399108887\n",
            "Total Pruning Time Consumed (mins): 6.285559240976969\n",
            "\n",
            "Pruned Model has size=29.90 MiB(non-zeros) = 84.92% of Original model size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pruned Model has Accuracy=92.07 % = -1.06% of Original model Accuracy\n",
            "\n",
            " \n",
            "==================== Fine-Tuning ========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1 Train Loss: 0.00000 Validation Accuracy: 92.83567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2 Train Loss: 0.00000 Validation Accuracy: 92.93587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:3 Train Loss: 0.00000 Validation Accuracy: 92.90581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:4 Train Loss: 0.00000 Validation Accuracy: 93.01603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:5 Train Loss: 0.00000 Validation Accuracy: 92.98597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Sparse model has size=29.90 MiB = 84.92% of Original model size\n",
            "Fine-Tuned Pruned Model Validation Accuracy: 92.98597194388778\n",
            "\n",
            " \n",
            "========================== Quantization-Aware Training(QAT) ===================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:  17%|█▋        | 17/98 [00:07<00:34,  2.35it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuCi_bPhNrMn"
      },
      "source": [
        "\n",
        "\n",
        "sconces = sconce()\n",
        "sconces.model= copy.deepcopy(model)\n",
        "sconces.criterion = nn.CrossEntropyLoss() # Loss\n",
        "sconces.optimizer= optim.Adam(sconces.model.parameters(), lr=1e-4)\n",
        "sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)\n",
        "sconces.dataloader = dataloader\n",
        "sconces.epochs = 1 #Number of time we iterate over the data\n",
        "sconces.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sconces.experiment_name = \"vgg-gmp\"\n",
        "sconces.prune_mode = \"GMP\" # Supports Automated Pruning Ratio Detection\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTSAWI8REuFZ"
      },
      "source": [
        "Easy function calls for Train and Validated the Model on the given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYNkCi7tC6Cl"
      },
      "source": [
        "# Train the model\n",
        "sconces.train()\n",
        "# Evaludate the model\n",
        "sconces.evaluate()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcOCi-1eFub4"
      },
      "source": [
        "Magic Happens here: Compress the model(GMP pruning is set as the prune mode[sconces.prune_mode] above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_dew86NADc"
      },
      "source": [
        "sconces.compress()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4Mmt3XLlHy"
      },
      "source": [
        "**Venum Pruning a better version of Wanda Pruning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-01T07:36:59.433154884Z",
          "start_time": "2023-12-01T07:33:58.232251482Z"
        },
        "id": "nQ1mWDluLlHy"
      },
      "source": [
        "# from sconce import sconce\n",
        "\n",
        "# sconces = sconce()\n",
        "# sconces.model = copy.deepcopy(model)\n",
        "# sconces.criterion = nn.CrossEntropyLoss()  # Loss\n",
        "# sconces.optimizer = optim.Adam(sconces.model.parameters(), lr=1e-4)\n",
        "# sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)\n",
        "# sconces.dataloader = dataloader\n",
        "# sconces.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# sconces.experiment_name = \"vgg-venum\"\n",
        "# sconces.prune_mode = \"venum\"  # Supports Automated Pruning Ratio Detection\n",
        "# sconces.compress()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJjAdrRK0aRD"
      },
      "source": [
        "Spiking Neural Network Compression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPatw1ex08kR"
      },
      "source": [
        "!pip install snntorch -q"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9qFLeGl0y24"
      },
      "source": [
        "# Import snntorch libraries\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import backprop\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "from snntorch import spikeplot as splt\n",
        "from torch import optim\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk0TCGvRKOWh"
      },
      "source": [
        "\n",
        "# Event Drive Data\n",
        "\n",
        "# dataloader arguments\n",
        "batch_size = 128\n",
        "data_path = \"./data/mnist\"\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Define a transform\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0,), (1,)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    mnist_train, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    mnist_test, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pj-ZzMQU9_Q"
      },
      "source": [
        "from sconce import sconce\n",
        "sconces = sconce()\n",
        "# Set you Dataloader\n",
        "dataloader = {}\n",
        "dataloader[\"train\"] = train_loader\n",
        "dataloader[\"test\"] = test_loader\n",
        "sconces.dataloader = dataloader"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzEHUCSx0hN6"
      },
      "source": [
        "#Enable snn in sconce\n",
        "sconces.snn = True\n",
        "\n",
        "# Load your snn Model\n",
        "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
        "beta = 0.5\n",
        "snn_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 12, 5),\n",
        "    nn.MaxPool2d(2),\n",
        "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "    nn.Conv2d(12, 64, 5),\n",
        "    nn.MaxPool2d(2),\n",
        "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 4 * 4, 10),\n",
        "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True),\n",
        ").to('cuda')\n",
        "\n",
        "\n",
        "#Load the pretrained weights\n",
        "snn_pretrained_model_path = \"drive/MyDrive/Efficientml/Efficientml.ai/snn_model.pth\"\n",
        "snn_model.load_state_dict(torch.load(snn_pretrained_model_path))  # Model Definition\n",
        "sconces.model = snn_model"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZLTwjV0VdJc"
      },
      "source": [
        "\n",
        "sconces.optimizer = optim.Adam(sconces.model.parameters(), lr=1e-4)\n",
        "sconces.scheduler = optim.lr_scheduler.CosineAnnealingLR(sconces.optimizer, T_max=200)\n",
        "\n",
        "sconces.criterion = SF.ce_rate_loss()\n",
        "\n",
        "sconces.epochs = 10  # Number of time we iterate over the data\n",
        "sconces.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sconces.experiment_name = \"snn-gmp\"  # Define your experiment name here\n",
        "sconces.prune_mode = \"GMP\"\n",
        "sconces.num_finetune_epochs = 1\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6kWCtSk0kVq"
      },
      "source": [
        "sconces.compress()"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}