{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center; font-weight: bold;\">SmoothQuant</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/smq_intuition.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mimages\u001b[0m/  QAT.ipynb      Quantization_compare_minmax_percentile.ipynb  SMQ.ipynb\n",
      "\u001b[01;34mPTQ\u001b[0m/     \u001b[01;34mquantization\u001b[0m/  Quantization-Implementation.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import functools\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Poor Mans NN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "calibration_data = DataLoader(mnist_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = NN()\n",
    "model.load_state_dict(torch.load(\"./linear_nn.pth\", weights_only=True)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 938/938 [00:04<00:00, 206.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_activations(model, calibration_data):\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    activations = {}\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        \"\"\"Update activation scales for the given tensor.\"\"\"\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\n",
    "        if name in activations:\n",
    "            activations[name] = torch.max(activations[name], comming_max)\n",
    "        else:\n",
    "            activations[name] = comming_max\n",
    "\n",
    "    def stat_input_hook(m, x, y, name):\n",
    "        \"\"\"Hook function to capture input activations.\"\"\"\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # In case x is a tuple, extract the first element.\n",
    "        stat_tensor(name, x)\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "\n",
    "    for images, labels in tqdm(calibration_data, desc=\"Processing batches\"):\n",
    "        inputs = images.to(device) \n",
    "        model(inputs)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    return activations\n",
    "\n",
    "\n",
    "# Fetch Activations for the Calibration Data\n",
    "activations = get_activations(model, calibration_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: linear1, Layer: Linear(in_features=784, out_features=128, bias=True), Activation: torch.Size([784])\n",
      "Layer Name: linear2, Layer: Linear(in_features=128, out_features=64, bias=True), Activation: torch.Size([128])\n",
      "Layer Name: linear3, Layer: Linear(in_features=64, out_features=10, bias=True), Activation: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    if name in activations:\n",
    "        print(f\"Layer Name: {name}, Layer: {layer}, Activation: {activations[name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Linear Layer Smoothing\n",
    "\n",
    "Now we see that `linear1` and `linear2` can be smoothed, as the input of `linear2` is the output of `linear1`.\n",
    "\n",
    "### Model Operations\n",
    "1. **Input fed into the Model:**\n",
    "   - `Xin`\n",
    "2. **Linear Layer 1:**\n",
    "   - `X1_out = Xin * W1.T + B1`\n",
    "3. **Linear Layer 2:**\n",
    "   - `X2_out = X1_out * W2.T + B2`\n",
    "4. **Linear Layer 3:**\n",
    "   - `Out = X2_out * W3.T + B3`\n",
    "\n",
    "### Smoothing Linear2\n",
    "To smoothen `linear2`, we derive activation scales from `X1_out` using the function `stat_tensor`. The process involves:\n",
    "\n",
    "1. **Compute Max Weight:**  \n",
    "   Use the function `smooth_linear_linear` to compute the maximum weight.\n",
    "\n",
    "2. **Compute Scale:**  \n",
    "   Use a scale factor `alpha = 0.5`.\n",
    "\n",
    "3. **Scale Adjustments:**\n",
    "   - Multiply the scale with `W2`.\n",
    "   - Divide the same scale with `W1`.\n",
    "\n",
    "Since the operations are associative, these transformations are mathematically equivalent.\n",
    "\n",
    "### Key Insight\n",
    "This process ensures smoother transitions between layers without altering the mathematical equivalence of the operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/smq_hardness_mitigation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](images/smq_example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](images/smq_formula.png)\n",
    "![image](images/smq_scale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that activation are difficult to quantize as their range and variance are quite high and are susceptible to the input fed.\n",
    "\n",
    "Hence, to mitigate this SmoothQuant postulates that the diffculty from the activation could be partially handed over to the weights of the previous layer.\n",
    "So that the difficutly is balanced betweent he weights and the activations. \n",
    "\n",
    "The main idea of SmoothQuant is to have Alpha = 0.5 for the scale such that the distribution of the difficulty is shared between the current layer weights and the activations.\n",
    "\n",
    "NOTE: The activations here mean that the scale is carried over the previous layer weight, thus having no overhead of the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def smooth_linear_linear(fc1, fc2, act_scales, alpha=0.5):\n",
    "    \"\"\"Apply weight smoothing to a pair of linear layers. \"\"\"\n",
    "    \n",
    "    assert isinstance(fc1, nn.Linear), \"fc1 must be an instance of nn.Linear\"\n",
    "    assert isinstance(fc2, nn.Linear), \"fc2 must be an instance of nn.Linear\"\n",
    "    assert fc1.out_features == fc2.in_features, \"fc1 out_features must match fc2 in_features\"\n",
    "    assert fc1.out_features == act_scales.numel(), \"Number of act_scales must match fc1 out_features\"\n",
    "\n",
    "    device, dtype = fc1.weight.device, fc1.weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "\n",
    "    # Compute weight scales per Output Feature Dimension\n",
    "    weight_scales = fc2.weight.abs().max(dim=0)[0]  # Max along Input Features Dimension\n",
    "    weight_scales = weight_scales.clamp(min=1e-5)  # Avoid division by zero\n",
    "\n",
    "    # print(\"Weight Scales Shape: \", weight_scales.shape)\n",
    "    # print(\"Activation Scales Shape: \", act_scales.shape)\n",
    "\n",
    "    # Calculate scales based on alpha\n",
    "    scales = (\n",
    "        (act_scales.pow(alpha) / weight_scales.pow(1 - alpha))\n",
    "        .clamp(min=1e-5)\n",
    "        .to(device=device, dtype=dtype)\n",
    "    )\n",
    "    # Weights are of shape (out_features, in_features), biases are of shape (out_features)\n",
    "    # For FC1, we divide the weights by the scales along the out_features, and the biases by the scales\n",
    "    # For FC2, we multiply the weights by the scales along the in_features\n",
    "\n",
    "    fc1.weight.div_(scales.view(-1, 1))  \n",
    "    if fc1.bias is not None:\n",
    "        fc1.bias.div_(scales)\n",
    "\n",
    "    fc2.weight.mul_(scales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing between Layers: Linear(in_features=784, out_features=128, bias=True) and Linear(in_features=128, out_features=64, bias=True)\n",
      "Smoothing between Layers: Linear(in_features=128, out_features=64, bias=True) and Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "previous_layer = None\n",
    "for name, current_layer in model.named_children():\n",
    "    if name in activations:   \n",
    "        if previous_layer is not None:\n",
    "            print(f\"Smoothing between Layers: {previous_layer} and {current_layer}\")\n",
    "            smooth_linear_linear(previous_layer, current_layer, activations[name], alpha=0.5)\n",
    "        previous_layer = current_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now Lets Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code borrowed from: https://github.com/mit-han-lab/smoothquant/blob/main/smoothquant/fake_quant.py\n",
    "# If you would like to understand how this can be implemented from scratch, please refer to my implementation here: https://github.com/satabios/quantization/quant/Weight_Only\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "class W8A8Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=8\n",
    "            )  # use 8-bit integer for weight\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=8\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): W8A8Linear(784, 128, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (relu): ReLU()\n",
      "  (linear2): W8A8Linear(128, 64, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (linear3): W8A8Linear(64, 10, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      ")\n",
      " ---------------- Quantizing Model Post Smoothing ---------------- \n",
      "Quantized Model: NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): W8A8Linear(784, 128, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (relu): ReLU()\n",
      "  (linear2): W8A8Linear(128, 64, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "  (linear3): W8A8Linear(64, 10, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Model: {model}\") \n",
    "print(\" ---------------- Quantizing Model Post Smoothing ---------------- \")\n",
    "for name, current_layer in model.named_children():\n",
    "    if isinstance(current_layer, nn.Linear):\n",
    "        model._modules[name] = W8A8Linear.from_float(current_layer, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_output=True)\n",
    "print(f\"Quantized Model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center; font-weight: bold;\">GPU Rich Bois!!... Peruse Further SmoothQuant on LLM</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm\n",
    "from transformers.models.mistral.modeling_mistral import (\n",
    "    MistralDecoderLayer,\n",
    "    MistralRMSNorm,\n",
    ")\n",
    "from transformers.models.mixtral.modeling_mixtral import (\n",
    "    MixtralDecoderLayer,\n",
    "    MixtralRMSNorm,\n",
    ")\n",
    "from transformers.models.opt.modeling_opt import (\n",
    "    OPTAttention,\n",
    "    OPTDecoderLayer,\n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers.models.falcon.modeling_falcon import FalconDecoderLayer\n",
    "\n",
    "from transformers.models.opt.modeling_opt import OPTPreTrainedModel\n",
    "from transformers.models.llama.modeling_llama import LlamaPreTrainedModel\n",
    "from transformers.models.mistral.modeling_mistral import MistralPreTrainedModel\n",
    "from transformers.models.mixtral.modeling_mixtral import MixtralPreTrainedModel\n",
    "from transformers.models.falcon.modeling_falcon import FalconPreTrainedModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center; font-weight: bold;\">SmoothQuant Helpers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quantize_opt(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = W8A8Linear.from_float(\n",
    "                m.fc1, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.fc2 = W8A8Linear.from_float(\n",
    "                m.fc2, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.out_proj = W8A8Linear.from_float(\n",
    "                m.out_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def quantize_llama_like(\n",
    "    model, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False\n",
    "):\n",
    "    from transformers.models.llama.modeling_llama import (\n",
    "        LlamaAttention,\n",
    "        LlamaMLP,\n",
    "    )\n",
    "\n",
    "    from transformers.models.mistral.modeling_mistral import (\n",
    "        MistralAttention,\n",
    "        MistralMLP,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, (LlamaMLP, MistralMLP)):\n",
    "            m.gate_proj = W8A8Linear.from_float(\n",
    "                m.gate_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.up_proj = W8A8Linear.from_float(\n",
    "                m.up_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.down_proj = W8A8Linear.from_float(\n",
    "                m.down_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "        elif isinstance(m, (LlamaAttention, MistralAttention)):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.o_proj = W8A8Linear.from_float(\n",
    "                m.o_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def quantize_mixtral(\n",
    "    model, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False\n",
    "):\n",
    "    from transformers.models.mixtral.modeling_mixtral import (\n",
    "        MixtralAttention,\n",
    "        MixtralSparseMoeBlock,\n",
    "        MixtralBLockSparseTop2MLP,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, MixtralBLockSparseTop2MLP):\n",
    "            m.w1 = W8A8Linear.from_float(\n",
    "                m.w1, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.w2 = W8A8Linear.from_float(\n",
    "                m.w2, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.w3 = W8A8Linear.from_float(\n",
    "                m.w3, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "        elif isinstance(m, MixtralAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.o_proj = W8A8Linear.from_float(\n",
    "                m.o_proj, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "        elif isinstance(m, MixtralSparseMoeBlock):\n",
    "            m.gate = W8A8Linear.from_float(\n",
    "                m.gate, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def quantize_falcon(\n",
    "    model, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=True\n",
    "):\n",
    "    from transformers.models.falcon.modeling_falcon import (\n",
    "        FalconAttention,\n",
    "        FalconMLP,\n",
    "    )\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, FalconMLP):\n",
    "            m.dense_h_to_4h = W8A8Linear.from_float(\n",
    "                m.dense_h_to_4h, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "            m.dense_4h_to_h = W8A8Linear.from_float(\n",
    "                m.dense_4h_to_h, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "        elif isinstance(m, FalconAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.query_key_value = W8A8Linear.from_float(\n",
    "                m.query_key_value,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.dense = W8A8Linear.from_float(\n",
    "                m.dense, weight_quant=weight_quant, act_quant=act_quant\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def quantize_model(\n",
    "    model, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_bmm_input=False\n",
    "):\n",
    "\n",
    "\n",
    "    if isinstance(model, OPTPreTrainedModel):\n",
    "        return quantize_opt(\n",
    "            model,\n",
    "            weight_quant=weight_quant,\n",
    "            act_quant=act_quant,\n",
    "            quantize_bmm_input=quantize_bmm_input,\n",
    "        )\n",
    "    elif isinstance(model, (LlamaPreTrainedModel, MistralPreTrainedModel)):\n",
    "        return quantize_llama_like(\n",
    "            model,\n",
    "            weight_quant=weight_quant,\n",
    "            act_quant=act_quant,\n",
    "            quantize_bmm_input=quantize_bmm_input,\n",
    "        )\n",
    "    elif isinstance(model, MixtralPreTrainedModel):\n",
    "        return quantize_mixtral(\n",
    "            model,\n",
    "            weight_quant=weight_quant,\n",
    "            act_quant=act_quant,\n",
    "            quantize_bmm_input=quantize_bmm_input,\n",
    "        )\n",
    "    elif isinstance(model, FalconPreTrainedModel):\n",
    "        return quantize_falcon(\n",
    "            model,\n",
    "            weight_quant=weight_quant,\n",
    "            act_quant=act_quant,\n",
    "            quantize_bmm_input=quantize_bmm_input,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {type(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs(ln, fcs, act_scales, alpha=0.5):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, nn.LayerNorm)\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "        assert ln.weight.numel() == fc.in_features == act_scales.numel()\n",
    "\n",
    "    device, dtype = fcs[0].weight.device, fcs[0].weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "    weight_scales = torch.cat(\n",
    "        [fc.weight.abs().max(dim=0, keepdim=True)[0] for fc in fcs], dim=0\n",
    "    )\n",
    "    weight_scales = weight_scales.max(dim=0)[0].clamp(min=1e-5)\n",
    "\n",
    "    scales = (\n",
    "        (act_scales.pow(alpha) / weight_scales.pow(1 - alpha))\n",
    "        .clamp(min=1e-5)\n",
    "        .to(device)\n",
    "        .to(dtype)\n",
    "    )\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs_llama_like(ln, fcs, act_scales, alpha=0.5):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, (LlamaRMSNorm, MistralRMSNorm, MixtralRMSNorm))\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "        assert ln.weight.numel() == fc.in_features == act_scales.numel()\n",
    "    device, dtype = fcs[0].weight.device, fcs[0].weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "    weight_scales = torch.cat(\n",
    "        [fc.weight.abs().max(dim=0, keepdim=True)[0] for fc in fcs], dim=0\n",
    "    )\n",
    "    weight_scales = weight_scales.max(dim=0)[0].clamp(min=1e-5)\n",
    "    scales = (\n",
    "        (act_scales.pow(alpha) / weight_scales.pow(1 - alpha))\n",
    "        .clamp(min=1e-5)\n",
    "        .to(device)\n",
    "        .to(dtype)\n",
    "    )\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_lm(model, scales, alpha=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            attn_ln = module.self_attn_layer_norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "            qkv_input_scales = scales[name + \".self_attn.q_proj\"]\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.final_layer_norm\n",
    "            fc1 = module.fc1\n",
    "            fc1_input_scales = scales[name + \".fc1\"]\n",
    "            smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)\n",
    "        elif isinstance(module, BloomBlock):\n",
    "            attn_ln = module.input_layernorm\n",
    "            qkv = module.self_attention.query_key_value\n",
    "            qkv_input_scales = scales[name + \".self_attention.query_key_value\"]\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.post_attention_layernorm\n",
    "            fc1 = module.mlp.dense_h_to_4h\n",
    "            fc1_input_scales = scales[name + \".mlp.dense_h_to_4h\"]\n",
    "            smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)\n",
    "        elif isinstance(module, FalconDecoderLayer):\n",
    "            qkv = module.self_attention.query_key_value\n",
    "            qkv_input_scales = scales[name + \".self_attention.query_key_value\"]\n",
    "            fc1_input_scales = scales[name + \".mlp.dense_h_to_4h\"]\n",
    "            fc1 = module.mlp.dense_h_to_4h\n",
    "\n",
    "            if (\n",
    "                not module.config.new_decoder_architecture\n",
    "                and module.config.parallel_attn\n",
    "            ):\n",
    "                attn_ln = module.input_layernorm\n",
    "                smooth_ln_fcs(attn_ln, [qkv, fc1], qkv_input_scales, alpha)\n",
    "            else:\n",
    "                attn_ln = (\n",
    "                    module.ln_attn\n",
    "                    if module.config.new_decoder_architecture\n",
    "                    else module.input_layernorm\n",
    "                )\n",
    "                ffn_ln = (\n",
    "                    module.ln_mlp\n",
    "                    if module.config.new_decoder_architecture\n",
    "                    else module.post_attention_layernorm\n",
    "                )\n",
    "                smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "                smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)\n",
    "        elif isinstance(module, (LlamaDecoderLayer, MistralDecoderLayer)):\n",
    "            attn_ln = module.input_layernorm  # attention forward norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "\n",
    "            qkv_input_scales = scales[name + \".self_attn.q_proj\"]\n",
    "            smooth_ln_fcs_llama_like(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.post_attention_layernorm  # feed forward norm\n",
    "            fcs = [module.mlp.gate_proj, module.mlp.up_proj]\n",
    "            fcs_input_scales = scales[name + \".mlp.gate_proj\"]\n",
    "\n",
    "            smooth_ln_fcs_llama_like(ffn_ln, fcs, fcs_input_scales, alpha)\n",
    "        elif isinstance(module, MixtralDecoderLayer):\n",
    "            attn_ln = module.input_layernorm  # attention forward norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "\n",
    "            qkv_input_scales = scales[name + \".self_attn.q_proj\"]\n",
    "            smooth_ln_fcs_llama_like(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.post_attention_layernorm  # feed forward norm\n",
    "            fcs = [module.block_sparse_moe.gate]\n",
    "            for expert in module.block_sparse_moe.experts:\n",
    "                fcs.append(expert.w1)\n",
    "                fcs.append(expert.w3)\n",
    "            fcs_input_scales = scales[name + \".block_sparse_moe.gate\"]\n",
    "\n",
    "            smooth_ln_fcs_llama_like(ffn_ln, fcs, fcs_input_scales, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. \n",
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same.\n",
    "\n",
    "In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the lm-eval-harness to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center; font-weight: bold;\">SmoothQuant</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples[\"text\"])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch[\"input_ids\"].to(self.device).unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "            last_token_logits = outputs.logits[:, -2, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "dataset = load_dataset(\"lambada\", split=\"validation[:1000]\")\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-1.3b\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) accuracy: 0.722\n"
     ]
    }
   ],
   "source": [
    "acc_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f\"Original model (fp16) accuracy: {acc_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance.\n",
    "\n",
    "Naive W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_w8a8 = quantize_opt(model_fp16)\n",
    "print(model_w8a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (v_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (q_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (out_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A8Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          (fc2): W8A8Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (v_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (q_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
       "            (out_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A8Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          (fc2): W8A8Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w8a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive W8A8 quantized model accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "del model_fp16\n",
    "acc_w8a8 = evaluator.evaluate(model_w8a8)\n",
    "print(f\"Naive W8A8 quantized model accuracy: {acc_w8a8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a significant accuracy drop. This is consistent with LLM.int8()'s finding: when the model size increases larger than 6.7B, systematic outliers will emerge in activations, which makes fully INT8 quantization impossible.\n",
    "\n",
    "SmoothQuant W8A8 Quantized Model Accuracy\n",
    "Let's smooth the model, quantize it, and check the performance! In ../act_scales, we provide the activation scales for OPT and BLOOM models. You can also use this notebook to test quantizing those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(2048, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(2048, 8192, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(8192, 2048, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-1.3b\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "act_scales = torch.load(\"opt-1.3b_smq_scales.pt\", weights_only=False)\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "model_smoothquant_w8a8 = quantize_opt(model)\n",
    "print(model_smoothquant_w8a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the smoothed model has the same accuracy as the FP16 model. This is because SmoothQuant smooths the outliers in activations and moves the quantization difficulty from activations to weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothQuant W8A8 quantized model accuracy: 0.707\n"
     ]
    }
   ],
   "source": [
    "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
    "print(f\"SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
